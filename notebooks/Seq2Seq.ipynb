{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Prime the GPU\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(session.run(hello))\n",
    "\n",
    "import pandas as pd\n",
    "from DeepText.preprocess import preProcessor_in_memory, presto2df\n",
    "pd.set_option('display.height', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REMOVE THIS LINE IF YOU ARE GOING TO PUT IN PRODUCTION\n",
    "df = pd.read_parquet('training_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1,400,000 rows and 9 columns in this data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo_homepage</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>issue_number</th>\n",
       "      <th>body</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>issue_title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mod_pagespeed</td>\n",
       "      <td>33489346</td>\n",
       "      <td>http://modpagespeed.com</td>\n",
       "      <td>66637566</td>\n",
       "      <td>677</td>\n",
       "      <td>entering this on behalf of the user who wishes...</td>\n",
       "      <td>mod_pagespeed causing high load on my server</td>\n",
       "      <td>7</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mod_pagespeed</td>\n",
       "      <td>33489346</td>\n",
       "      <td>http://modpagespeed.com</td>\n",
       "      <td>66637570</td>\n",
       "      <td>678</td>\n",
       "      <td>not able to find the file cacheflush in the sp...</td>\n",
       "      <td>automatically make cacheflush file on install ...</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mod_pagespeed</td>\n",
       "      <td>33489346</td>\n",
       "      <td>http://modpagespeed.com</td>\n",
       "      <td>66637575</td>\n",
       "      <td>679</td>\n",
       "      <td>you can still set a location handler for the b...</td>\n",
       "      <td>document new beacon handling logic in next rel...</td>\n",
       "      <td>8</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mod_pagespeed</td>\n",
       "      <td>33489346</td>\n",
       "      <td>http://modpagespeed.com</td>\n",
       "      <td>66637576</td>\n",
       "      <td>680</td>\n",
       "      <td>right now flatten_css_imports requires rewrite...</td>\n",
       "      <td>if flatten_css_imports is enabled without rewr...</td>\n",
       "      <td>11</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mod_pagespeed</td>\n",
       "      <td>33489346</td>\n",
       "      <td>http://modpagespeed.com</td>\n",
       "      <td>66637578</td>\n",
       "      <td>681</td>\n",
       "      <td>what steps will reproduce the problem\\n\\nin my...</td>\n",
       "      <td>mod_pagespeed dont respect errorlevel settings...</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       repo_name   repo_id            repo_homepage  issue_id  issue_number  \\\n",
       "0  mod_pagespeed  33489346  http://modpagespeed.com  66637566           677   \n",
       "1  mod_pagespeed  33489346  http://modpagespeed.com  66637570           678   \n",
       "2  mod_pagespeed  33489346  http://modpagespeed.com  66637575           679   \n",
       "3  mod_pagespeed  33489346  http://modpagespeed.com  66637576           680   \n",
       "4  mod_pagespeed  33489346  http://modpagespeed.com  66637578           681   \n",
       "\n",
       "                                                body  \\\n",
       "0  entering this on behalf of the user who wishes...   \n",
       "1  not able to find the file cacheflush in the sp...   \n",
       "2  you can still set a location handler for the b...   \n",
       "3  right now flatten_css_imports requires rewrite...   \n",
       "4  what steps will reproduce the problem\\n\\nin my...   \n",
       "\n",
       "                                         issue_title  issue_title_len  \\\n",
       "0       mod_pagespeed causing high load on my server                7   \n",
       "1  automatically make cacheflush file on install ...               10   \n",
       "2  document new beacon handling logic in next rel...                8   \n",
       "3  if flatten_css_imports is enabled without rewr...               11   \n",
       "4  mod_pagespeed dont respect errorlevel settings...                8   \n",
       "\n",
       "   body_len  \n",
       "0        87  \n",
       "1        29  \n",
       "2        69  \n",
       "3        78  \n",
       "4        75  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT repo_name, repo_id, repo_homepage, issue_id, issue_number, \n",
    "        body, \n",
    "        issue_title,\n",
    "        cardinality(split(issue_title, ' ')) as issue_title_len,\n",
    "        cardinality(split(body, ' ')) as body_len\n",
    "        \n",
    "FROM(\n",
    "SELECT  \n",
    "   r.name as repo_name\n",
    " , r.homepage as repo_homepage\n",
    " , i.repository_id as repo_id\n",
    " , i.id as issue_id\n",
    " , i.number as issue_number\n",
    " , substr(trim(regexp_replace(lower(from_utf8(i.body)), '<[^>]*>|[^a-zA-Z_\\s]')), 1, 500) as body\n",
    " , trim(regexp_replace(lower(from_utf8(i.title)), '<[^>]*>|[^a-zA-Z_\\s]')) as issue_title\n",
    "FROM hive.snapshots_presto.issues as i\n",
    "JOIN hive.entities.repos as r on \n",
    "    i.repository_id = r.repo_id and\n",
    "    r.num_stars >= 50 and \n",
    "    r.num_issues >= 10 and \n",
    "    r.num_prs >= 10 and \n",
    "    r.num_merged_pr_authors >= 3 and \n",
    "    r.num_issue_authors >= 3 and not\n",
    "    r.is_fork and \n",
    "    r.num_fetchers_last_30_days >= 2 and\n",
    "    r.is_public\n",
    "WHERE i.body is not null and i.title is not null\n",
    ") a\n",
    "WHERE cardinality(split(issue_title, ' ', 5)) = 5 AND cardinality(split(body, ' ', 10)) = 10\n",
    "limit 1400000\n",
    "\"\"\"\n",
    "\n",
    "# YOU WANT TO UNCOMMENT THIS LINE WHEN PUTTING IN PRODUCTION.  I JUST CACHED THE RESULTS INSTEAD\n",
    "#df = presto2df(query)\n",
    "n_rows, n_columns = df.shape\n",
    "print(f'There are {n_rows:,} rows and {n_columns} columns in this data.')\n",
    "data_to_clean_body = df.body.tolist()\n",
    "data_to_clean_title = df.issue_title.tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UNCOMMENT THIS LINE WHEN YOU PUT INTO PRODUCTION\n",
    "#df.to_parquet('training_data.parquet', engine='fastparquet', compression='UNCOMPRESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 root  root  604M Dec 13 20:44 training_data.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah | grep .parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of what the body of an issue looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entering this on behalf of the user who wishes to remain anonymous\\n\\nwhat steps will reproduce the problem\\n turning on mod_pagespeed and browsing the website\\n new site on dedicated server there are no other visitors besides me load \\naverage is usually  to  turning on mod_pagespeed and browsing a few \\npages results in load average going up to  to  if i dont restart apache \\nand continue browsing this goes up to  and crashes server\\n\\nwhat version of the product are you using please check xmodpagespee'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_clean_body[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of what the title of that same issue looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mod_pagespeed causing high load on my server'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_clean_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Word -> Idx Mapping\n",
    "\n",
    "`ppm.fit_transform` performs the following steps using all threads available:\n",
    "\n",
    "1.  Cleans each document as using default cleaning function (from textacy) or one that you set.\n",
    "2.  Tokenizes each document using defualt tokenizer (spacy-english) or one that you set.\n",
    "3.  Builds a vocabulary of all tokens and builds a mapping from each token -> integer index.  Extremely rare tokens are excluded and an additional place is reserved for new or unseen words.\n",
    "4.  Applies the token -> integer mapping over all of the documents, and applies padding and truncating to bring each document to a consistent width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:built corpus by parsing 1,400,000 documents\n"
     ]
    }
   ],
   "source": [
    "#specifying the huerestic and the maxlen yourself makes this run faster the utility doesn't have to calculate these for you.\n",
    "ppm_body = preProcessor_in_memory(hueristic_pct=.61, keep_n=6000, maxlen=60)\n",
    "vectorized_body = ppm_body.fit_transform(data_to_clean_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as dpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpickle.dump(ppm_body, open( \"ppm_body.dpkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5,  6,  7,  8,  9,  1, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         7, 18, 19, 20,  4,  1, 21, 22,  7, 23, 19, 24, 25,  4, 26, 27, 28,\n",
       "        29, 30, 31,  1, 32, 33, 34, 35, 36, 37, 10, 20,  4,  1, 21, 22, 38,\n",
       "        39, 40, 41, 42, 34, 35, 43, 44, 10]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_body[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see statistics on document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5,  6,  7,  8,  9,  1, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         7, 18, 19, 20,  4,  1, 21, 22,  7, 23, 19, 24, 25,  4, 26, 27, 28,\n",
       "        29, 30, 31,  1, 32, 33, 34, 35, 36, 37, 10, 20,  4,  1, 21, 22, 38,\n",
       "        39, 40, 41, 42, 34, 35, 43, 44, 10]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_body[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Vectorized Data To Original\n",
    "\n",
    "We want to check that these numbers are correct.  How do we do that?  There are dictionaries attached to the `preProcessor_in_memory` class that give idx -> word mappings and word -> index mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entering this on behalf of the user who wishes to remain anonymous\\n\\nwhat steps will reproduce the problem\\n turning on mod_pagespeed and browsing the website\\n new site on dedicated server there are no other visitors besides me load \\naverage is usually  to  turning on mod_pagespeed and browsing a few \\npages results in load average going up to  to  if i dont restart apache \\nand continue browsing this goes up to  and crashes server\\n\\nwhat version of the product are you using please check xmodpagespee',\n",
       " 'not able to find the file cacheflush in the specified at \\nmodpagespeedfilecachepath\\n\\noperating system is linux\\napache \\nmps version \\n\\n\\n\\n\\noriginal issue reported on codegooglecom by somanathgmailcom on  apr  at']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_clean_body[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Vector back to to text with the following exceptions:\n",
    "\n",
    "- 0: is just padding\n",
    "- 1: is reserved for unseen or rare wordds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Example :\n",
      "=====\n",
      " ['entering', 'this', 'on', 'behalf', 'of', 'the', 'user', 'who', 'to', 'remain', 'anonymous', '\\n', 'what', 'steps', 'will', 'reproduce', 'the', 'problem', '\\n ', 'turning', 'on', 'and', 'browsing', 'the', 'website', '\\n ', 'new', 'site', 'on', 'dedicated', 'server', 'there', 'are', 'no', 'other', 'besides', 'me', 'load', 'average', 'is', 'usually', 'to', 'turning', 'on', 'and', 'browsing', 'a', 'few', 'pages', 'results', 'in', 'load', 'average', 'going', 'up', 'to']\n",
      "\n",
      "Second Example :\n",
      "=====\n",
      " ['not', 'able', 'to', 'find', 'the', 'file', 'in', 'the', 'specified', 'at', '\\n', 'operating', 'system', 'is', 'linux', '\\n', 'apache', 'version', 'original', 'issue', 'reported', 'on', 'codegooglecom', 'by', 'on', 'apr', 'at']\n"
     ]
    }
   ],
   "source": [
    "print('First Example :\\n=====\\n', [ppm_body.id2token[x] for x in vectorized_body[0] if x > 1])\n",
    "\n",
    "print('\\nSecond Example :\\n=====\\n', [ppm_body.id2token[x] for x in vectorized_body[1] if x > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also for sanity check, lets confirm the most common tokens that the parser found in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1048898</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>967821</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>951024</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>740399</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>683265</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count token\n",
       "5   1048898   the\n",
       "11   967821    \\n\n",
       "8    951024    to\n",
       "36   740399     a\n",
       "34   683265    is"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppm_body.token_count_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process Issue Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created this for an issue body, lets do the same for the issue titles, which will have a seperate integer mapping.  In this case, we want to append <Start> and <End> indicators around the titles so our sequence to sequence model can work properly and let us know when it thinks the issue title should End. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:built corpus by parsing 1,400,000 documents\n"
     ]
    }
   ],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# from DeepText.preprocess import preProcessor_in_memory\n",
    "ppm_title = preProcessor_in_memory(hueristic_pct=.99, append_indicators=True, padding='post', keep_n=4000, maxlen=12)\n",
    "vectorized_title = ppm_title.fit_transform(data_to_clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpickle.dump(ppm_title, open( \"ppm_title.dpkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1400000</td>\n",
       "      <td>_start_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1400000</td>\n",
       "      <td>_end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>275175</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>204193</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>179587</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count    token\n",
       "0   1400000  _start_\n",
       "7   1400000    _end_\n",
       "63   275175       to\n",
       "20   204193       in\n",
       "12   179587      for"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppm_title.token_count_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the <Start> and <End> indicators are the most common tokens in the title space as they are appended to every title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the Issue Body and Issue Title numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of issue body array: (1400000, 60)\n",
      "shape of issue title array: (1400000, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of issue body array: {vectorized_body.shape}')\n",
    "print(f'shape of issue title array: {vectorized_title.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Arrays\n",
    "import numpy as np\n",
    "np.save('body_vectors.npy', vectorized_body)\n",
    "np.save('title_vectors.npy', vectorized_title)\n",
    "\n",
    "# Save id <-> token mappings\n",
    "title_id2token = ppm_title.id2token\n",
    "title_token2id = ppm_title.token2id\n",
    "dpickle.dump(title_id2token, open( \"title_id2token.dpkl\", \"wb\" ) )\n",
    "dpickle.dump(title_token2id, open( \"title_token2id.dpkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 root  root  321M Dec 15 01:14 body_vectors.npy\r\n",
      "-rw-r--r--  1 root  root   65M Dec 15 01:14 title_vectors.npy\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lah | grep .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 root  root  993M Dec 15 00:35 ppm_body.dpkl\r\n",
      "-rw-r--r--  1 root  root  173M Dec 15 01:13 ppm_title.dpkl\r\n",
      "-rw-r--r--  1 root  root   76K Dec 15 01:14 title_id2token.dpkl\r\n",
      "-rw-r--r--  1 root  root   76K Dec 15 01:14 title_token2id.dpkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah | grep .dpkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data (You Can Re-Run From Here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectorized_body = np.load('body_vectors.npy')\n",
    "vectorized_title = np.load('title_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "with open('ppm_body.dpkl', 'rb') as file:\n",
    "    ppm_body = dpickle.load(file)\n",
    "    \n",
    "with open('ppm_title.dpkl', 'rb') as file:\n",
    "    ppm_title = dpickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( \"title_id2token.dpkl\", \"rb\" ) as file:\n",
    "    title_id2token = dpickle.load(file)\n",
    "    \n",
    "with open( \"title_token2id.dpkl\", \"rb\" ) as file:\n",
    "    title_token2id = dpickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning On This Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder input: (1400000, 11)\n",
      "Shape of decoder target: (1400000, 11)\n",
      "Shape of encoder input: (1400000, 60)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# For Decoder Input, you don't need the last word as that is only for prediction \n",
    "# when we are training using Teacher Forcing.  \n",
    "decoder_input_data = vectorized_title[:, :-1]\n",
    "\n",
    "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
    "#decoder_target_data = to_categorical(vectorized_title[:, 1:])\n",
    "decoder_target_data = vectorized_title[:, 1:]\n",
    "\n",
    "# Encoder input is simply the body of the issue text\n",
    "encoder_input_data = vectorized_body\n",
    "\n",
    "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
    "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
    "print(f'Shape of encoder input: {encoder_input_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of encoder word embedding (Issue Body): 6,002\n",
      "Dimensionality of decoder word (Title) and output layer: 4,002\n",
      "Dimensionality of encoder hidden state: 300\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = max(ppm_body.id2token.keys()) + 1\n",
    "num_decoder_tokens = max(ppm_title.id2token.keys()) + 1\n",
    "latent_dim = 300\n",
    "\n",
    "print(f'Dimensionality of encoder word embedding (Issue Body): {num_encoder_tokens:,}')\n",
    "print(f'Dimensionality of decoder word (Title) and output layer: {num_decoder_tokens:,}')\n",
    "print(f'Dimensionality of encoder hidden state: {latent_dim:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Seq -> Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really simple architecture:  a minimial seq -> seq setup, without attention and no bi-directional LSTM with very few dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def free_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "free_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "body-input (InputLayer)         (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Body-Word-Embedding (Embedding) (None, 60, 300)      1800600     body-input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "title-input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60, 300)      1200        Body-Word-Embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Title-Word-Embedding (Embedding (None, None, 300)    1200600     title-input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Last (GRU)              [(None, 300), (None, 540900      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (GRU)                   [(None, None, 300),  540900      Title-Word-Embedding[0][0]       \n",
      "                                                                 Encoder-Last[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 300)    1200        Decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output-Layer (Dense)            (None, None, 4002)   1204602     batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 5,290,002\n",
      "Trainable params: 5,288,802\n",
      "Non-trainable params: 1,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Experiment####\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(encoder_input_data.shape[1],), name='body-input')\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization()(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Bidirectional(GRU(latent_dim//2, name='Encoder-Bi-GRU', return_sequences=True))(x)\n",
    "#x = BatchNormalization()(x)\n",
    "encoder_outputs, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last')(x)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,), name='title-input')\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Title-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder')\n",
    "decoder_gru_output, _ = decoder_gru(dec_emb, initial_state=state_h)\n",
    "x = BatchNormalization()(decoder_gru_output)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Output-Layer')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 570.00 483.00\" width=\"570pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-479 566,-479 566,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139792830711400 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139792830711400</title>\n",
       "<polygon fill=\"none\" points=\"75,-438.5 75,-474.5 257,-474.5 257,-438.5 75,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-452.8\">body-input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139792830711512 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139792830711512</title>\n",
       "<polygon fill=\"none\" points=\"30,-365.5 30,-401.5 302,-401.5 302,-365.5 30,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-379.8\">Body-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139792830711400&#45;&gt;139792830711512 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139792830711400-&gt;139792830711512</title>\n",
       "<path d=\"M166,-438.313C166,-430.289 166,-420.547 166,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"169.5,-411.529 166,-401.529 162.5,-411.529 169.5,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792830710392 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139792830710392</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 332,-328.5 332,-292.5 0,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-306.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139792830711512&#45;&gt;139792830710392 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139792830711512-&gt;139792830710392</title>\n",
       "<path d=\"M166,-365.313C166,-357.289 166,-347.547 166,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"169.5,-338.529 166,-328.529 162.5,-338.529 169.5,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792830710056 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139792830710056</title>\n",
       "<polygon fill=\"none\" points=\"350.5,-292.5 350.5,-328.5 527.5,-328.5 527.5,-292.5 350.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"439\" y=\"-306.8\">title-input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139792828882728 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139792828882728</title>\n",
       "<polygon fill=\"none\" points=\"294,-219.5 294,-255.5 562,-255.5 562,-219.5 294,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"428\" y=\"-233.8\">Title-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139792830710056&#45;&gt;139792828882728 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139792830710056-&gt;139792828882728</title>\n",
       "<path d=\"M436.337,-292.313C435.094,-284.289 433.585,-274.547 432.194,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"435.628,-264.875 430.638,-255.529 428.711,-265.947 435.628,-264.875\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792830711064 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139792830711064</title>\n",
       "<polygon fill=\"none\" points=\"99.5,-219.5 99.5,-255.5 254.5,-255.5 254.5,-219.5 99.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-233.8\">Encoder-Last: GRU</text>\n",
       "</g>\n",
       "<!-- 139792830710392&#45;&gt;139792830711064 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139792830710392-&gt;139792830711064</title>\n",
       "<path d=\"M168.663,-292.313C169.906,-284.289 171.415,-274.547 172.806,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"176.289,-265.947 174.362,-255.529 169.372,-264.875 176.289,-265.947\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792828882672 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139792828882672</title>\n",
       "<polygon fill=\"none\" points=\"242.5,-146.5 242.5,-182.5 361.5,-182.5 361.5,-146.5 242.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-160.8\">Decoder: GRU</text>\n",
       "</g>\n",
       "<!-- 139792828882728&#45;&gt;139792828882672 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139792828882728-&gt;139792828882672</title>\n",
       "<path d=\"M397.82,-219.494C380.956,-209.991 359.74,-198.036 341.561,-187.792\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"342.801,-184.474 332.371,-182.614 339.364,-190.572 342.801,-184.474\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792830711064&#45;&gt;139792828882672 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139792830711064-&gt;139792828882672</title>\n",
       "<path d=\"M206.941,-219.494C223.67,-209.991 244.718,-198.036 262.753,-187.792\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"264.904,-190.596 271.87,-182.614 261.447,-184.509 264.904,-190.596\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792822721168 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139792822721168</title>\n",
       "<polygon fill=\"none\" points=\"136,-73.5 136,-109.5 468,-109.5 468,-73.5 136,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-87.8\">batch_normalization_2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139792828882672&#45;&gt;139792822721168 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139792828882672-&gt;139792822721168</title>\n",
       "<path d=\"M302,-146.313C302,-138.289 302,-128.547 302,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"305.5,-119.529 302,-109.529 298.5,-119.529 305.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792823134808 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139792823134808</title>\n",
       "<polygon fill=\"none\" points=\"219,-0.5 219,-36.5 385,-36.5 385,-0.5 219,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-14.8\">Output-Layer: Dense</text>\n",
       "</g>\n",
       "<!-- 139792822721168&#45;&gt;139792823134808 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139792822721168-&gt;139792823134808</title>\n",
       "<path d=\"M302,-73.3129C302,-65.2895 302,-55.5475 302,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"305.5,-46.5288 302,-36.5288 298.5,-46.5289 305.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Brief thoughts on how this model can improve from tuning the parameters\n",
    "\n",
    " I found a positive correlation b/w the following items and accuracy\n",
    " \n",
    " - The learning rate can be decreased.\n",
    " - The latent dimension should be increased.  Right now its 300 lets try 600\n",
    " - Right now only training on 1.4M rows of data.  I can scale this up to 10M rows really easily and that should dramatically improve accuracy.\n",
    " - I haven't gone through the process of making the model overfit and backing it off until it is \"just right\" this is a key step that I skipped because the model was \"good enough\" for MVP.  \n",
    " \n",
    " \n",
    " Other optimizations I should do:\n",
    " \n",
    " - Add Attention Layer\n",
    " - Stack RNNs instead of just having a single layer for both encoder and decoder. \n",
    " - Better tokenization of text -> right now its very suboptimal.  Areas where I'm getting tokenization wrong.  Also not handling code gracefully.  \n",
    "\n",
    " \n",
    " Things I tried that didn't work initially but should try again:\n",
    " \n",
    " - Bi-directional RNN -> Added more capacity but didn't seem to increase accuracy.  I should try skip connection - Have a stack that reads the text forwards and backwards and concatenates that to the encoding vector.\n",
    " \n",
    " Things I haven't tried at all:\n",
    " \n",
    " - Pre-train word embeddings instead of learning them from scratch (My intution is this shouldn't work well for this problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1190000 samples, validate on 210000 samples\n",
      "Epoch 1/10\n",
      "1190000/1190000 [==============================] - 179s 150us/step - loss: 3.0874 - val_loss: 2.7067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer Decoder was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Last/while/Exit_2:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.6040 - val_loss: 2.5945\n",
      "Epoch 3/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.4991 - val_loss: 2.5555\n",
      "Epoch 4/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.4393 - val_loss: 2.5351\n",
      "Epoch 5/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.3972 - val_loss: 2.5254\n",
      "Epoch 6/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.3648 - val_loss: 2.5240\n",
      "Epoch 7/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.3378 - val_loss: 2.5233\n",
      "Epoch 8/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.3153 - val_loss: 2.5275\n",
      "Epoch 9/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.2955 - val_loss: 2.5304\n",
      "Epoch 10/10\n",
      "1190000/1190000 [==============================] - 176s 148us/step - loss: 2.2778 - val_loss: 2.5344\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'v2_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 10\n",
    "history = model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.15, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 root  root   61M Dec 15 01:21 v2_seq2seq.epoch01-val2.70675.hdf5\r\n",
      "-rw-r--r--  1 root  root   61M Dec 15 01:24 v2_seq2seq.epoch02-val2.59445.hdf5\r\n",
      "-rw-r--r--  1 root  root   61M Dec 15 01:27 v2_seq2seq.epoch03-val2.55547.hdf5\r\n",
      "-rw-r--r--  1 root  root   61M Dec 15 01:30 v2_seq2seq.epoch04-val2.53507.hdf5\r\n",
      "-rw-r--r--  1 root  root   61M Dec 15 01:33 v2_seq2seq.epoch05-val2.52536.hdf5\r\n",
      "-rw-r--r--  1 root  root   61M Dec 15 01:36 v2_seq2seq.epoch06-val2.52396.hdf5\r\n",
      "-rw-r--r--  1 root  root   61M Dec 15 01:39 v2_seq2seq.epoch07-val2.52329.hdf5\r\n",
      "-rw-r--r--  1 root  root   326 Dec 15 01:47 v2_seq2seq.log\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lah | grep v2_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22c30019e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_model_training_history(history_object):\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim([2.2, 3.0])\n",
    "    plt.plot(history_object.history['loss'])\n",
    "    plt.plot(history_object.history['val_loss'])\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is overfitting - should add some regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Create Encoder, Simple\n",
    "def extract_encoder_model():\n",
    "    \"\"\"This is really straight forward, just grab the correct input and output layer from original model\n",
    "       For the encoder, there is only one input layer and one output layer, no looping or multiple inputs.\n",
    "    \"\"\"\n",
    "    #TODO: clean this up once your are confident to take as input model object\n",
    "    \n",
    "    encoder_model = Model(encoder_inputs, state_h)\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_decoder_model():\n",
    "    \"\"\"\n",
    "    Extract the decoder from the training network.  This is a little tricky so I made some notes in comments.\n",
    "    \"\"\"\n",
    "    #TODO: clean this up once your are confident to take as input model object\n",
    "    \n",
    "    # Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
    "    # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
    "    # the GRU, thus we define this input layer for the state so we can add this capability\n",
    "    d_gru_inference_state_input = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # we need to reuse the weights that is why we are getting this\n",
    "    d_gru_inference = model.get_layer('Decoder')\n",
    "    # If you inspect the decoder GRU that we created for training, it will take as input \n",
    "    # 2 tensors -> (1) is the embedding layer output for the teacher forcing \n",
    "    #                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
    "    #              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
    "    #                   grab the state after the first prediction and feed that back in again.\n",
    "    d_gru_out, d_gru_state_out = d_gru_inference([dec_emb, d_gru_inference_state_input])\n",
    "\n",
    "\n",
    "    d_dense_out = decoder_dense(d_gru_out)\n",
    "    d_model_inference = Model([decoder_inputs, d_gru_inference_state_input], \n",
    "                              [d_dense_out, d_gru_state_out])\n",
    "    return d_model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title-input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Title-Word-Embedding (Embedding (None, None, 300)    1200600     title-input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (GRU)                   [(None, None, 300),  540900      Title-Word-Embedding[0][0]       \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output-Layer (Dense)            (None, None, 4002)   1204602     Decoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,946,102\n",
      "Trainable params: 2,946,102\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model_inference = extract_decoder_model()\n",
    "decoder_model_inference.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 454.00 264.00\" width=\"454pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 450,-260 450,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139792830710056 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139792830710056</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-219.5 45.5,-255.5 222.5,-255.5 222.5,-219.5 45.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-233.8\">title-input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139792828882728 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139792828882728</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 268,-182.5 268,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-160.8\">Title-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139792830710056&#45;&gt;139792828882728 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139792830710056-&gt;139792828882728</title>\n",
       "<path d=\"M134,-219.313C134,-211.289 134,-201.547 134,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-192.529 134,-182.529 130.5,-192.529 137.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792828882672 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139792828882672</title>\n",
       "<polygon fill=\"none\" points=\"190.5,-73.5 190.5,-109.5 309.5,-109.5 309.5,-73.5 190.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250\" y=\"-87.8\">Decoder: GRU</text>\n",
       "</g>\n",
       "<!-- 139792828882728&#45;&gt;139792828882672 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139792828882728-&gt;139792828882672</title>\n",
       "<path d=\"M161.785,-146.494C177.167,-137.079 196.484,-125.255 213.117,-115.075\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"215.338,-117.819 222.04,-109.614 211.683,-111.849 215.338,-117.819\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139787246747096 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139787246747096</title>\n",
       "<polygon fill=\"none\" points=\"286,-146.5 286,-182.5 446,-182.5 446,-146.5 286,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-160.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139787246747096&#45;&gt;139792828882672 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139787246747096-&gt;139792828882672</title>\n",
       "<path d=\"M338.215,-146.494C322.833,-137.079 303.516,-125.255 286.883,-115.075\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"288.317,-111.849 277.96,-109.614 284.662,-117.819 288.317,-111.849\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792823134808 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139792823134808</title>\n",
       "<polygon fill=\"none\" points=\"167,-0.5 167,-36.5 333,-36.5 333,-0.5 167,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250\" y=\"-14.8\">Output-Layer: Dense</text>\n",
       "</g>\n",
       "<!-- 139792828882672&#45;&gt;139792823134808 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139792828882672-&gt;139792823134808</title>\n",
       "<path d=\"M250,-73.3129C250,-65.2895 250,-55.5475 250,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"253.5,-46.5288 250,-36.5288 246.5,-46.5289 253.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(decoder_model_inference).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "body-input (InputLayer)      (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "Body-Word-Embedding (Embeddi (None, 60, 300)           1800600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 60, 300)           1200      \n",
      "_________________________________________________________________\n",
      "Encoder-Last (GRU)           [(None, 300), (None, 300) 540900    \n",
      "=================================================================\n",
      "Total params: 2,342,700\n",
      "Trainable params: 2,342,100\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model_inference = extract_encoder_model()\n",
    "encoder_model_inference.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 340.00 264.00\" width=\"340pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 336,-260 336,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139792830711400 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139792830711400</title>\n",
       "<polygon fill=\"none\" points=\"75,-219.5 75,-255.5 257,-255.5 257,-219.5 75,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-233.8\">body-input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139792830711512 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139792830711512</title>\n",
       "<polygon fill=\"none\" points=\"30,-146.5 30,-182.5 302,-182.5 302,-146.5 30,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-160.8\">Body-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139792830711400&#45;&gt;139792830711512 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139792830711400-&gt;139792830711512</title>\n",
       "<path d=\"M166,-219.313C166,-211.289 166,-201.547 166,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"169.5,-192.529 166,-182.529 162.5,-192.529 169.5,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792830710392 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139792830710392</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 332,-109.5 332,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-87.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139792830711512&#45;&gt;139792830710392 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139792830711512-&gt;139792830710392</title>\n",
       "<path d=\"M166,-146.313C166,-138.289 166,-128.547 166,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"169.5,-119.529 166,-109.529 162.5,-119.529 169.5,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139792830711064 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139792830711064</title>\n",
       "<polygon fill=\"none\" points=\"88.5,-0.5 88.5,-36.5 243.5,-36.5 243.5,-0.5 88.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-14.8\">Encoder-Last: GRU</text>\n",
       "</g>\n",
       "<!-- 139792830710392&#45;&gt;139792830711064 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139792830710392-&gt;139792830711064</title>\n",
       "<path d=\"M166,-73.3129C166,-65.2895 166,-55.5475 166,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"169.5,-46.5288 166,-36.5288 162.5,-46.5289 169.5,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(encoder_model_inference).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Encoder and Decoder Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer Decoder was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_1:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "decoder_model_inference.save('decoder_model_inference.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model_inference.save('encoder_model_inference.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test If Serialized Models Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/models.py:252: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_model_loaded = load_model('encoder_model_inference.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model_inference.predict(encoder_input_data[0:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/models.py:252: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "decoder_model_loaded = load_model('decoder_model_inference.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_issue_title_vOriginal(raw_input_text, stochastic=False, max_len_title=12):\n",
    "    # Seed For _start_ token\n",
    "    raw_tokenized = ppm_body.transform_text([raw_input_text])\n",
    "    body_encoding = encoder_model_inference.predict(raw_tokenized)\n",
    "    state_value = np.array(title_token2id['_start_']).reshape(1, 1)\n",
    "\n",
    "    decoded_sentence = []\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        preds, st = decoder_model_inference.predict([state_value, body_encoding])\n",
    "\n",
    "        # This is cheating because we are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
    "        #  Argmax will return the integer index corresponding to the prediction + 2 b/c we chopped off first two\n",
    "        pred_idx = np.argmax(preds[:,:,2:]) + 2\n",
    "\n",
    "        if stochastic is True:\n",
    "            normalized_probs = preds[0,0,2:] / np.sum(preds[0,0,2:])\n",
    "            integer_idxs = np.arange(2, max(title_id2token.keys()) + 1)\n",
    "            pred_idx = np.random.choice(integer_idxs, p = normalized_probs)\n",
    "        \n",
    "        # retrieve word from index prediction\n",
    "        pred_word_str = title_id2token[pred_idx]\n",
    "\n",
    "        \n",
    "\n",
    "        if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n",
    "            stop_condition = True\n",
    "            break\n",
    "        decoded_sentence.append(pred_word_str)\n",
    "        \n",
    "        #update the decoder for the next word\n",
    "        body_encoding = st\n",
    "        state_value = np.array(pred_idx).reshape(1, 1)\n",
    "        \n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_issue_title_vLoaded(raw_input_text, stochastic=False, max_len_title=12):\n",
    "    # Seed For _start_ token\n",
    "    raw_tokenized = ppm_body.transform_text([raw_input_text])\n",
    "    body_encoding = encoder_model_loaded.predict(raw_tokenized)\n",
    "    state_value = np.array(title_token2id['_start_']).reshape(1, 1)\n",
    "\n",
    "    decoded_sentence = []\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        preds, st = decoder_model_loaded.predict([state_value, body_encoding])\n",
    "\n",
    "        # This is cheating because we are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
    "        #  Argmax will return the integer index corresponding to the prediction + 2 b/c we chopped off first two\n",
    "        pred_idx = np.argmax(preds[:,:,2:]) + 2\n",
    "\n",
    "        if stochastic is True:\n",
    "            normalized_probs = preds[0,0,2:] / np.sum(preds[0,0,2:])\n",
    "            integer_idxs = np.arange(2, max(title_id2token.keys()) + 1)\n",
    "            pred_idx = np.random.choice(integer_idxs, p = normalized_probs)\n",
    "        \n",
    "        # retrieve word from index prediction\n",
    "        pred_word_str = title_id2token[pred_idx]\n",
    "\n",
    "        \n",
    "\n",
    "        if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n",
    "            stop_condition = True\n",
    "            break\n",
    "        decoded_sentence.append(pred_word_str)\n",
    "        \n",
    "        #update the decoder for the next word\n",
    "        body_encoding = st\n",
    "        state_value = np.array(pred_idx).reshape(1, 1)\n",
    "        \n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi everyone\\n\\ni am facing below issue where i am not able to perform save operation in configure section of code repo when i click on save button system does not respond and it stays as it is \\n\\nfrom other threads i found that api and collector needs to be run for this i have executed both of them and both of them are running properly\\n\\ncould you please help me here\\n\\ni have pasted my properties file below for your reference\\n database name\\n\\nspringdatamongodbdatabasedashboarddb\\n database hostname  de'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 910023\n",
    "raw_input_text=data_to_clean_body[i]\n",
    "raw_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unable to save pull request in save method in ui'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_issue_title_vOriginal(raw_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unable to save pull request in save method in ui'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_issue_title_vLoaded(raw_input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! Yes it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Inferencing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now just taking this from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_issue_title = generate_issue_title_vLoaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Examples  (This is The Best Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_examples_from_data(i):\n",
    "    print(f'\\n\\n ======= Example # {i} ==========\\n')\n",
    "    raw_input_text=data_to_clean_body[i]\n",
    "\n",
    "    print('Issue Body:\\n', raw_input_text, '\\n')\n",
    "\n",
    "    print('Title:\\n', data_to_clean_title[i])\n",
    "\n",
    "    print('\\n******Prediction*******:\\n', generate_issue_title(raw_input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_list = np.random.randint(low = 1, high =len(data_to_clean_body), size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ======= Example # 492431 ==========\n",
      "\n",
      "Issue Body:\n",
      " i need to add start up animation on pie chart and and could not find way to do this \n",
      "\n",
      "Title:\n",
      " animation on pie charts\n",
      "\n",
      "******Prediction*******:\n",
      " how can i start pie chart on start and add new animation\n",
      "\n",
      "\n",
      " ======= Example # 620066 ==========\n",
      "\n",
      "Issue Body:\n",
      " ive been using circ for a couple weeks since buying a chromebook and its a pretty client but theres a bunch of things that need fixing that ive seen and a stack of unanswered issues i dont really expect anyone else to pay attention to this ticket im just making a point in case anyone wants to show this project some love i might even be the person doing the work if this ticket gets closed then i guess someones paying attention anyway\n",
      "\n",
      "so is this something that might happen eventually or should i  \n",
      "\n",
      "Title:\n",
      " needs a solid week of dedication and polish\n",
      "\n",
      "******Prediction*******:\n",
      " fixing issue with creating a new ticket that causes the client to\n",
      "\n",
      "\n",
      " ======= Example # 826608 ==========\n",
      "\n",
      "Issue Body:\n",
      " on ios iphone\n",
      "in kitchen sink range polyfill section after a while the slider stop following the finger and are offset  dont match the speed \n",
      "\n",
      "Title:\n",
      " range polyfill knob is offset from finger\n",
      "\n",
      "******Prediction*******:\n",
      " polyfill range polyfill breaks slider\n",
      "\n",
      "\n",
      " ======= Example # 849275 ==========\n",
      "\n",
      "Issue Body:\n",
      " from the activity example\n",
      "\n",
      "\n",
      "csnotificationview note  \n",
      "noteshowingactivity  yes\n",
      "\n",
      "note setvisibleyes animatedyes completionnil\n",
      "\n",
      "note dismisswithstylecsnotificationviewstylesuccess messagesuccess\n",
      "          durationkcsnotificationviewdefaultshowduration animatedyes\n",
      "\n",
      "\n",
      "if i created the view with _progressnotification  csnotificationview alloc initwithparentviewcontroller_topvc i am unable to set the textlabel \n",
      "\n",
      "Title:\n",
      " cannot modify textlabel for activity example from readme\n",
      "\n",
      "******Prediction*******:\n",
      " unable to create a new view when creating a new view\n",
      "\n",
      "\n",
      " ======= Example # 1304645 ==========\n",
      "\n",
      "Issue Body:\n",
      " i use aa on a android library project but find that it doesnt  generate any code how can i fix it \n",
      "\n",
      "Title:\n",
      " aa doesnt work on android library project\n",
      "\n",
      "******Prediction*******:\n",
      " how can i find the code that works with the project\n",
      "\n",
      "\n",
      " ======= Example # 15803 ==========\n",
      "\n",
      "Issue Body:\n",
      " allow commaseparated values for nicmac nicvlan and tapdev variables\n",
      "\n",
      "open issues\n",
      " the autogenerated numbers for tap devices could be improved\n",
      " vde support is not yet done with n workers x m interfaces it is easy to exceed the number of ports\n",
      " do we still need the old variable multinet \n",
      "\n",
      "Title:\n",
      " support for multiple networks in one job\n",
      "\n",
      "******Prediction*******:\n",
      " allow more flexible configuration values for the new ui and the new\n",
      "\n",
      "\n",
      " ======= Example # 667356 ==========\n",
      "\n",
      "Issue Body:\n",
      " hey there i read your wordpress development blog post and set up this environment on my box\n",
      "\n",
      "i am quite happy with it there are some possible tweaks in the scripts that i will look into later but there is one thing that bugs me in my comprehension this is clearly an image with development in mind not productive usage so for securityprivacy reasons i would and did it here only open the ports on my local machine like\n",
      "\n",
      "\n",
      "wordpress\n",
      "  ports\n",
      "       \n",
      "       \n",
      "\n",
      "\n",
      "and\n",
      "\n",
      "\n",
      "mysql\n",
      "  ports\n",
      "     \n",
      "\n",
      "\n",
      "because by defa \n",
      "\n",
      "Title:\n",
      " development or productive image\n",
      "\n",
      "******Prediction*******:\n",
      " development environments and development environments bug fixes\n",
      "\n",
      "\n",
      " ======= Example # 169680 ==========\n",
      "\n",
      "Issue Body:\n",
      " documentation issue  the wiki page titled object mapping httpsgithubcomrestkitrestkitwikiobjectmapping needs a section on using rkentitymappings addconnectionforrelationship method  its a common json data situation thats not covered at all  \n",
      "\n",
      "i struggled through the wiki page examples searched google github and stackoverflow tried multiple combinations of guess code  i finally found the addconnectionforrelationship method mentioned  its such a simple rk client solution to the fk problem\n",
      "\n",
      "ps than \n",
      "\n",
      "Title:\n",
      " add foreign key example to object mapping wiki\n",
      "\n",
      "******Prediction*******:\n",
      " improve documentation about supporting objects in the client api\n",
      "\n",
      "\n",
      " ======= Example # 1339335 ==========\n",
      "\n",
      "Issue Body:\n",
      " handlers are retrieved from the application but there is no trace of them being ever set lets get them from the config \n",
      "\n",
      "Title:\n",
      " posthandlers are not invoked for google and likely for linkedin\n",
      "\n",
      "******Prediction*******:\n",
      " remove invalid flag from handlers\n",
      "\n",
      "\n",
      " ======= Example # 1118918 ==========\n",
      "\n",
      "Issue Body:\n",
      " please provide updated docs for  at wkhtmltopdforgusagewkhtmltopdftxt\n",
      "\n",
      "missing flags with default value documentation\n",
      "bypassproxyfor \n",
      "keeprelativelinks \n",
      "resolverelativelinks\n",
      "\n",
      "thanks for your great work \n",
      "\n",
      "Title:\n",
      " update autogenerated docs wkhtmltopdftxt\n",
      "\n",
      "******Prediction*******:\n",
      " provide documentation for default flag in docs\n",
      "\n",
      "\n",
      " ======= Example # 1255968 ==========\n",
      "\n",
      "Issue Body:\n",
      " please let me know what should be the expected behavior \n",
      "\n",
      "Title:\n",
      " empty notebook wont export as r source\n",
      "\n",
      "******Prediction*******:\n",
      " add option to disable empty string concat method\n",
      "\n",
      "\n",
      " ======= Example # 685593 ==========\n",
      "\n",
      "Issue Body:\n",
      " q  a \n",
      "    \n",
      " doc fix  yes \n",
      " new docs  no \n",
      " applies to   \n",
      " fixed tickets   \n",
      "\n",
      "i have upgraded the french page about security from top to about line \n",
      "\n",
      "some text are not yet translated but put in place diffing was a big task \n",
      "\n",
      "do i need to ask a pull request for the master branch too \n",
      "\n",
      "Title:\n",
      " sync of french translation from top to line\n",
      "\n",
      "******Prediction*******:\n",
      " improve the french translation of the new content view\n",
      "\n",
      "\n",
      " ======= Example # 894201 ==========\n",
      "\n",
      "Issue Body:\n",
      " i am thinking about the implementation of dumping eav attributes in to_array\n",
      "\n",
      "what do you think \n",
      "\n",
      "Title:\n",
      " dump eav to array\n",
      "\n",
      "******Prediction*******:\n",
      " how about supporting the new features of the data attributes\n",
      "\n",
      "\n",
      " ======= Example # 1267456 ==========\n",
      "\n",
      "Issue Body:\n",
      " i am using the atompreviewhttpsgithubcomglavinatompreview package  plugin it has a keyboard shortcut to show and hide the preview pane unfortunately it doesnt show me the shortcut when i am searching for it with control  shift  p like other packages i am using ive created an issue for that httpsgithubcomglavinatompreviewissues and the author of the package told me to ask the question here\n",
      "\n",
      "could you tell me why atom doesnt display the command ctrlalty\n",
      "\n",
      "screenshothttpscloudgithubusercontentcomass \n",
      "\n",
      "Title:\n",
      " package doesnt show keyboard shortcut\n",
      "\n",
      "******Prediction*******:\n",
      " tbg feature request shortcut conflict between two package control panels\n",
      "\n",
      "\n",
      " ======= Example # 12258 ==========\n",
      "\n",
      "Issue Body:\n",
      " similar to the add instrumentation filter it would be nice to have a general \n",
      "purpose way to inject a beacon of any type whether it be inlined or thirdparty\n",
      "\n",
      "i guess you could use the canonicalize_javascript_libraries filter to \n",
      "accomplish the same thing but seems hacky\n",
      "\n",
      "generally this would makes for some interesting usecases\n",
      "\n",
      "\n",
      "original issue reported on codegooglecom by hayesgmailcom on  jun  at \n",
      "\n",
      "Title:\n",
      " enable general purpose beacon injection\n",
      "\n",
      "******Prediction*******:\n",
      " rfc add a way to inject a custom filter injection into the\n",
      "\n",
      "\n",
      " ======= Example # 910095 ==========\n",
      "\n",
      "Issue Body:\n",
      " i havent managed to figure out exactly whats causing this one but ive had it happen two times before but forgot to take screenshots but it definitely occurs within the new theme settings i shall investigate more\n",
      "\n",
      "screenshot_httpscloudgithubusercontentcomassetsafccefeaeapng\n",
      "screenshot_httpscloudgithubusercontentcomassetsaedececebfpng\n",
      "screenshot_httpscloudgithubusercontentcomassetsafaeceaecdbbfabdpng \n",
      "\n",
      "Title:\n",
      " its possible to set main colour to none\n",
      "\n",
      "******Prediction*******:\n",
      " intermittent loss of theme when creating new theme\n",
      "\n",
      "\n",
      " ======= Example # 458684 ==========\n",
      "\n",
      "Issue Body:\n",
      " reporter eastoraaslilawduluthcom\n",
      "\n",
      "move the to on the number line robert peter leali\n",
      "rpjlealiaolcom \n",
      "ph      \n",
      "settlement now  take school seriously here or west\n",
      "\n",
      "fractions_on_the_number_line_htmlseedproblemhttpsandcastlekhanacademyorgmediacastleskhanmasterexercisesfractions_on_the_number_line_htmlseedproblemdebug\n",
      "\n",
      "answer timelinehttpsandcastlekhanacademyorgmediacastleskhanmasterexercisesfractions_on_the_number_line_htmlseedproblemdebugactivitybbcorrectactivityccdd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mozilla compatible msie  wi \n",
      "\n",
      "Title:\n",
      " fractions on the number line   instructions\n",
      "\n",
      "******Prediction*******:\n",
      " fractions on the number line the number line is not moving the\n",
      "\n",
      "\n",
      " ======= Example # 269891 ==========\n",
      "\n",
      "Issue Body:\n",
      " when running tarbell newproject and tarbell update tarbell tries to get a branch of the base template that matches the tarbell version string eg for version b tarbell expects httpsgithubcomnewsappstarbelltemplatetreeb this was implemented to keep new versions of the base template from causing problems with old tarbell versions but it needs to go  our small audience is on the latest version \n",
      "\n",
      "now that were approaching release we should move to something more flexible  a system that tracks master  \n",
      "\n",
      "Title:\n",
      " proposal support any commit ref for base template and perhaps default to master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******Prediction*******:\n",
      " spurious spurious update template includes old version of the new base branch\n",
      "\n",
      "\n",
      " ======= Example # 980081 ==========\n",
      "\n",
      "Issue Body:\n",
      " we work with a lot of java and scala stacktraces and the other options ive tried for supporting them in heka dont work as well as id like this is an implementation of a regexbased multilinesplitter which works great for our stacktraces the implementation is that you define a regex to use as the delimiter and a regex used to match lines that should be joined together it first splits the buffer using the delimiter and then checks each section against the multiline regex to see if its a match all l \n",
      "\n",
      "Title:\n",
      " implementation of a multiline splitter\n",
      "\n",
      "******Prediction*******:\n",
      " support for custom delimiter handling in the raw delimiter handling\n",
      "\n",
      "\n",
      " ======= Example # 615760 ==========\n",
      "\n",
      "Issue Body:\n",
      " i wanted to display an error class on the invalid inputs only after the form was submitted but this doesnt seem to be possible just yet\n",
      "also its convenient to have access to a reset method to clear all the inputs\n",
      "i can add more test if youre cool with the idea \n",
      "\n",
      "Title:\n",
      " dont validate the inputs when the form was not submitted\n",
      "\n",
      "******Prediction*******:\n",
      " invalid form handling invalid inputs\n",
      "\n",
      "\n",
      " ======= Example # 547059 ==========\n",
      "\n",
      "Issue Body:\n",
      " is there an option to just see if the code compiles without flashing any chips  this would be helpful for people who are testing what code does and doesnt get past the compile phase  \n",
      "\n",
      "i hope i havent overlooked an option to do this \n",
      "\n",
      "Title:\n",
      " feature  run compileonly\n",
      "\n",
      "******Prediction*******:\n",
      " feature request add option to disable the entire build process\n",
      "\n",
      "\n",
      " ======= Example # 1328865 ==========\n",
      "\n",
      "Issue Body:\n",
      " when installing ushahidi  and using hebrew characters for the sites name and tagline  they end up becoming jibrish due to encoding error \n",
      "\n",
      "Title:\n",
      " during installation can not use hebrew characters for sites name\n",
      "\n",
      "******Prediction*******:\n",
      " invalid characters in the installer when using the new characters in the\n",
      "\n",
      "\n",
      " ======= Example # 812794 ==========\n",
      "\n",
      "Issue Body:\n",
      " some vertex buffers may be created without a gl context and it would be convenient if this buffer only allocated its gl_array_buffer when bindvbo is called\n",
      "\n",
      "fix incoming \n",
      "\n",
      "Title:\n",
      " osdcpuglvertexbuffer lazy gl allocation\n",
      "\n",
      "******Prediction*******:\n",
      " avoid unnecessary gl buffer when called multiple times\n",
      "\n",
      "\n",
      " ======= Example # 752870 ==========\n",
      "\n",
      "Issue Body:\n",
      " i installed the git extension on my  brackets and i get this on startup\n",
      "\n",
      " html\n",
      "expectederror no git has been found on this computer\n",
      "\n",
      "searched paths\n",
      "git\n",
      "usrlocalgitbingit\n",
      "usrlocalbingit\n",
      "usrbingit\n",
      "\n",
      "\n",
      "yet in a terminal i can see that git is installed\n",
      "\n",
      " bash\n",
      " whereis git\n",
      "git usrbingit usrsharemanmangitgz \n",
      "\n",
      "Title:\n",
      " expectederror no git has been found on this computer\n",
      "\n",
      "******Prediction*******:\n",
      " unable to locate git installed on a fresh install\n",
      "\n",
      "\n",
      " ======= Example # 193967 ==========\n",
      "\n",
      "Issue Body:\n",
      " i wanted to point a coworker to a specific detect so i went looking for the id in the markup and added it to the url like httpsmodernizrcomdownloadsetclassesemoji but there is some url rewriting happening that turns the link into httpsmodernizrcomdownloademojisetclasses\n",
      "\n",
      "it would be really useful to be able to share a detect in this way obviously not a super important feature but nice to have \n",
      "\n",
      "Title:\n",
      " support for linking to a detect and its description\n",
      "\n",
      "******Prediction*******:\n",
      " detect disconnected when a share is detected\n",
      "\n",
      "\n",
      " ======= Example # 859046 ==========\n",
      "\n",
      "Issue Body:\n",
      " goodie pull request template\n",
      "\n",
      "using this template will help us better understand your instant answer and assist you when necessary  simply copy and paste the markdown below into the description of your github pull request and complete as appropriate\n",
      "\n",
      "what does your instant answer do\n",
      "it says that im awesome\n",
      "\n",
      "what problem does your instant answer solve why is it better than organic links\n",
      "the problem is that nobody knows that im awesome\n",
      "\n",
      "what is the data source for your instant answer provide a lin \n",
      "\n",
      "Title:\n",
      " created my first goodie\n",
      "\n",
      "******Prediction*******:\n",
      " created my own pull request for the new google instant answer\n",
      "\n",
      "\n",
      " ======= Example # 109061 ==========\n",
      "\n",
      "Issue Body:\n",
      " rely on serverside validation instead as email and password do anyway \n",
      "\n",
      "Title:\n",
      " remove username check on registration form\n",
      "\n",
      "******Prediction*******:\n",
      " remove unnecessary clientside validation check for invalid email address\n",
      "\n",
      "\n",
      " ======= Example # 191375 ==========\n",
      "\n",
      "Issue Body:\n",
      " reporter objohnkenobigmailcom\n",
      "\n",
      "shouldnt a become a\n",
      "there is no value in this problem that should turn the exponent of a from a positive to a negative\n",
      "at least not one that i can see\n",
      "love the site\n",
      "thanks\n",
      "\n",
      "simplifying_expressions_with_exponentshtmlseedproblemsimplifyexponentialformminihttpsandcastlekhanacademyorgmediacastleskhanmasterexercisessimplifying_expressions_with_exponentshtmlseedproblemsimplifyexponentialformminidebug\n",
      "\n",
      "answer timelinehttpsandcastlekhanacademyorgmediacastleskhanmasterexerc \n",
      "\n",
      "Title:\n",
      " simplifying expressions with exponents  exponents and radicals\n",
      "\n",
      "******Prediction*******:\n",
      " negative number word problems the answer is wrong\n",
      "\n",
      "\n",
      " ======= Example # 430986 ==========\n",
      "\n",
      "Issue Body:\n",
      " labtohex_comphttpsfcloudgithubcomassetsfeaebefbbfbpng\n",
      "the hexadecimal color that results from lab to hex string conversion and the hex string that photoshop has for the lab values are slightly diferents but still noticeable\n",
      "\n",
      "this is the lab picked in photoshop and the hex value for that \n",
      "\n",
      " javascript\n",
      "l  b  a\n",
      "fc\n",
      "\n",
      "\n",
      "during the conversion from the above lab to the string in hex code with d numbers are round up and that change the hex resulting in a lab value slightly brighter\n",
      "\n",
      " javascript\n",
      "d          \n",
      "\n",
      "Title:\n",
      " lab to string conversion  round numbers doubt\n",
      "\n",
      "******Prediction*******:\n",
      " improve performance of the new string representation for the new string changes\n",
      "\n",
      "\n",
      " ======= Example # 237080 ==========\n",
      "\n",
      "Issue Body:\n",
      " mapped audio rate bus in osc message refplied for g_querytree message\n",
      "was referring to control rate bus fixed this \n",
      "\n",
      "Title:\n",
      " server mapped audio bus for g_querytreereply\n",
      "\n",
      "******Prediction*******:\n",
      " improve bus message bus mapped in new client\n"
     ]
    }
   ],
   "source": [
    "for i in demo_list:\n",
    "    show_examples_from_data(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox To Play With Input Text and Get Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "@register_cell_magic\n",
    "def summarize_issue(line, cell):\n",
    "    answer = generate_issue_title(cell)\n",
    "    print('\\n******Prediction*******:\\n',answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******Prediction*******:\n",
      " add support for continuous scaling\n"
     ]
    }
   ],
   "source": [
    "%%summarize_issue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test Fit Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class threadsafe_iter(object):\n",
    "  \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "  def __init__(self, it):\n",
    "      self.it = it\n",
    "      self.lock = threading.Lock()\n",
    "\n",
    "  def __iter__(self):\n",
    "      return self\n",
    "\n",
    "  def __next__(self):\n",
    "      with self.lock:\n",
    "          return self.it.__next__()\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "  \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "  def g(*a, **kw):\n",
    "      return threadsafe_iter(f(*a, **kw))\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import time\n",
    "\n",
    "\n",
    "arr_data = np.random.randint(0, 256, (50000, 2))\n",
    "\n",
    "arr_labels = np.random.randint(0, 2, 50000)\n",
    "\n",
    "@threadsafe_generator\n",
    "def custom_generator(use_weights=False):\n",
    "        batch_size = 10\n",
    "        n_samples = arr_data.shape[0]\n",
    "\n",
    "        while True:\n",
    "            batch_index = np.random.randint(0, n_samples - batch_size)\n",
    "            start = batch_index\n",
    "            end = start + batch_size\n",
    "            X = arr_data[start: end]\n",
    "            y = arr_labels[start: end]\n",
    "            time.sleep(1) # simulate i/olag from disk\n",
    "            yield X, y\n",
    "\n",
    "# Build a NN\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2, )))\n",
    "model.compile(loss='mse', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10/10 [==============================] - 10s 1s/step - loss: 7718.3467\n",
      "CPU times: user 304 ms, sys: 76 ms, total: 380 ms\n",
      "Wall time: 18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd91146e438>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# use normal threading\n",
    "model.fit_generator(custom_generator(),\n",
    "                    steps_per_epoch=10,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    max_queue_size=10,\n",
    "                    workers=8,\n",
    "                    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training.py:2057: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 7118.5013\n",
      "CPU times: user 100 ms, sys: 772 ms, total: 872 ms\n",
      "Wall time: 2.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7dd8b9e48>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# use process based threading, duplicates data\n",
    "model.fit_generator(custom_generator(),\n",
    "                    steps_per_epoch=10,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    max_queue_size=10,\n",
    "                    workers=8,\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10/10 [==============================] - 10s 1s/step - loss: 6720.5703\n",
      "CPU times: user 204 ms, sys: 48 ms, total: 252 ms\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5775a3a58>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#no parallelism\n",
    "model.fit_generator(custom_generator(),\n",
    "                    steps_per_epoch=10,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    max_queue_size=1,\n",
    "                    workers=1,\n",
    "                    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_data = np.array([range(500)]).T\n",
    "arr_labels = arr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from time import sleep\n",
    "import bcolz\n",
    "lock = threading.Lock()\n",
    "\n",
    "a = Input(shape=(1,))\n",
    "model = Model(inputs=a, outputs=a)\n",
    "model.compile(loss='mse', optimizer='adadelta')\n",
    "\n",
    "start = 0\n",
    "\n",
    "@threadsafe_generator\n",
    "def custom_generator(use_weights=False):\n",
    "        batch_size = 10\n",
    "        n_samples = arr_data.shape[0]\n",
    "\n",
    "        while True:\n",
    "            batch_index = 1\n",
    "            start = batch_index\n",
    "            end = start + batch_size\n",
    "            X = arr_data[start: end]\n",
    "            y = arr_labels[start: end]\n",
    "            time.sleep(.01) # simulate i/olag from disk\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 32 ms, total: 188 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test1 = model.predict_generator(custom_generator(), steps = 100, workers=2, \n",
    "                               use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 20 ms, total: 96 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test2 = model.predict_generator(custom_generator(), steps = 100, \n",
    "                               workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class Custom_Seq(Sequence):\n",
    "\n",
    "    def __init__(self, x_set=arr_data, y_set=arr_labels, batch_size=10):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        time.sleep(.1)\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs = Custom_Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.97 s, total: 2.97 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test3 = model.predict_generator(cs, steps=100, workers=10, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Single vs. Parallel Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarking 500,000 rows on 8 cores.\n"
     ]
    }
   ],
   "source": [
    "benchmark_n = 500000\n",
    "from multiprocessing import cpu_count\n",
    "n_cores = cpu_count()\n",
    "print(f'benchmarking {benchmark_n:,} rows on {n_cores} cores.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Threaded Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 s, sys: 536 ms, total: 42.4 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_single = ppm.process_text(data_to_clean[:benchmark_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'getlog', 'methods', 'on', 'services', 'are'],\n",
       " ['done', 'the', 'migration', 'from', 'hepmcgenparticle', 'to'],\n",
       " ['here', 'is', 'my', 'template', 'code', 'for']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[:6] for x in results_single[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Threaded Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.92 s, sys: 936 ms, total: 2.86 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_parallel = ppm.parallel_process_text(data_to_clean[:benchmark_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'getlog', 'methods', 'on', 'services', 'are'],\n",
       " ['done', 'the', 'migration', 'from', 'hepmcgenparticle', 'to'],\n",
       " ['here', 'is', 'my', 'template', 'code', 'for']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[:6] for x in results_parallel[0][:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Instantiate A Magic Function for Jupyter Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "@register_cell_magic\n",
    "def sql(line, cell):\n",
    "    return presto2df(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner_id</th>\n",
       "      <th>owner_type</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>name</th>\n",
       "      <th>repo_nwo</th>\n",
       "      <th>description</th>\n",
       "      <th>homepage</th>\n",
       "      <th>repo_created_at</th>\n",
       "      <th>primary_language_id</th>\n",
       "      <th>is_fork</th>\n",
       "      <th>num_public_forks</th>\n",
       "      <th>is_public</th>\n",
       "      <th>has_wiki</th>\n",
       "      <th>open_source_license</th>\n",
       "      <th>num_stars</th>\n",
       "      <th>num_issues</th>\n",
       "      <th>num_prs</th>\n",
       "      <th>issue_authors</th>\n",
       "      <th>pr_authors</th>\n",
       "      <th>merged_pr_authors</th>\n",
       "      <th>num_issue_authors</th>\n",
       "      <th>num_pr_authors</th>\n",
       "      <th>num_merged_pr_authors</th>\n",
       "      <th>first_issue_created_at</th>\n",
       "      <th>first_pr_created_at</th>\n",
       "      <th>last_issue_created_at</th>\n",
       "      <th>last_pr_created_at</th>\n",
       "      <th>last_pr_merged_at</th>\n",
       "      <th>num_fetches_last_30_days</th>\n",
       "      <th>fetchers_last_30_days</th>\n",
       "      <th>num_fetchers_last_30_days</th>\n",
       "      <th>first_push_at</th>\n",
       "      <th>last_push_at</th>\n",
       "      <th>num_pushes_last_30_days</th>\n",
       "      <th>num_commits_last_30_days</th>\n",
       "      <th>committers_last_30_days</th>\n",
       "      <th>num_committers_last_30_days</th>\n",
       "      <th>num_views_last_30_days</th>\n",
       "      <th>logged_in_viewers_last_30_days</th>\n",
       "      <th>num_logged_in_viewers_last_30_days</th>\n",
       "      <th>pct_threads_with_assignee</th>\n",
       "      <th>num_protected_branches</th>\n",
       "      <th>protected_branch_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40116050</td>\n",
       "      <td>2</td>\n",
       "      <td>blogers/2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-03 02:18:48.000</td>\n",
       "      <td>2015-08-03 02:18:48.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40116027</td>\n",
       "      <td>194</td>\n",
       "      <td>blogers/194</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-03 02:18:18.000</td>\n",
       "      <td>2015-08-03 02:18:18.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40115960</td>\n",
       "      <td>180</td>\n",
       "      <td>blogers/180</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[12613545]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-03 02:17:01.000</td>\n",
       "      <td>2015-08-03 02:17:01.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40115892</td>\n",
       "      <td>171</td>\n",
       "      <td>blogers/171</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-03 02:16:10.000</td>\n",
       "      <td>2015-08-03 02:16:10.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40115825</td>\n",
       "      <td>160</td>\n",
       "      <td>blogers/160</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-03 02:15:10.000</td>\n",
       "      <td>2015-08-03 02:15:10.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40115712</td>\n",
       "      <td>134</td>\n",
       "      <td>blogers/134</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[12613545]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-03 02:12:42.000</td>\n",
       "      <td>2015-08-03 02:12:42.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40115622</td>\n",
       "      <td>12</td>\n",
       "      <td>blogers/12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[12613545]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-03 02:11:22.000</td>\n",
       "      <td>2015-08-03 02:11:22.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13621688</td>\n",
       "      <td>User</td>\n",
       "      <td>40115577</td>\n",
       "      <td>110</td>\n",
       "      <td>blogers/110</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[12613545]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-03 02:10:33.000</td>\n",
       "      <td>2015-08-03 02:10:33.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13622183</td>\n",
       "      <td>User</td>\n",
       "      <td>86450634</td>\n",
       "      <td>liaokongvfx.github.io</td>\n",
       "      <td>liaokongVFX/liaokongvfx.github.io</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>140.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[None]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-28 04:12:57.000</td>\n",
       "      <td>2017-03-28 04:14:10.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[None, 34088524]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13622329</td>\n",
       "      <td>User</td>\n",
       "      <td>107088488</td>\n",
       "      <td>thundercracker</td>\n",
       "      <td>fishbaoz/thundercracker</td>\n",
       "      <td>b\"The software toolchain for Sifteo's 2nd gene...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>149.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[32748793]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   owner_id owner_type    repo_id                   name  \\\n",
       "0  13621688       User   40116050                      2   \n",
       "1  13621688       User   40116027                    194   \n",
       "2  13621688       User   40115960                    180   \n",
       "3  13621688       User   40115892                    171   \n",
       "4  13621688       User   40115825                    160   \n",
       "5  13621688       User   40115712                    134   \n",
       "6  13621688       User   40115622                     12   \n",
       "7  13621688       User   40115577                    110   \n",
       "8  13622183       User   86450634  liaokongvfx.github.io   \n",
       "9  13622329       User  107088488         thundercracker   \n",
       "\n",
       "                            repo_nwo  \\\n",
       "0                          blogers/2   \n",
       "1                        blogers/194   \n",
       "2                        blogers/180   \n",
       "3                        blogers/171   \n",
       "4                        blogers/160   \n",
       "5                        blogers/134   \n",
       "6                         blogers/12   \n",
       "7                        blogers/110   \n",
       "8  liaokongVFX/liaokongvfx.github.io   \n",
       "9            fishbaoz/thundercracker   \n",
       "\n",
       "                                         description homepage repo_created_at  \\\n",
       "0                                               None     None      2015-08-03   \n",
       "1                                               None     None      2015-08-03   \n",
       "2                                               None     None      2015-08-03   \n",
       "3                                               None     None      2015-08-03   \n",
       "4                                               None     None      2015-08-03   \n",
       "5                                               None     None      2015-08-03   \n",
       "6                                               None     None      2015-08-03   \n",
       "7                                               None     None      2015-08-03   \n",
       "8                                               None     None      2017-03-28   \n",
       "9  b\"The software toolchain for Sifteo's 2nd gene...     None      2017-10-15   \n",
       "\n",
       "   primary_language_id  is_fork  num_public_forks  is_public  has_wiki  \\\n",
       "0                192.0    False                 0       True      True   \n",
       "1                192.0    False                 0       True      True   \n",
       "2                192.0    False                 0       True      True   \n",
       "3                192.0    False                 0       True      True   \n",
       "4                192.0    False                 0       True      True   \n",
       "5                192.0    False                 0       True      True   \n",
       "6                192.0    False                 0       True      True   \n",
       "7                  NaN    False                 0       True      True   \n",
       "8                140.0    False                 0       True      True   \n",
       "9                149.0     True                 0       True      True   \n",
       "\n",
       "  open_source_license num_stars num_issues num_prs issue_authors pr_authors  \\\n",
       "0                None      None       None    None          None       None   \n",
       "1                None      None       None    None          None       None   \n",
       "2                None      None       None    None          None       None   \n",
       "3                None      None       None    None          None       None   \n",
       "4                None      None       None    None          None       None   \n",
       "5                None      None       None    None          None       None   \n",
       "6                None      None       None    None          None       None   \n",
       "7                None      None       None    None          None       None   \n",
       "8                None      None       None    None          None       None   \n",
       "9                None      None       None    None          None       None   \n",
       "\n",
       "  merged_pr_authors num_issue_authors num_pr_authors num_merged_pr_authors  \\\n",
       "0              None              None           None                  None   \n",
       "1              None              None           None                  None   \n",
       "2              None              None           None                  None   \n",
       "3              None              None           None                  None   \n",
       "4              None              None           None                  None   \n",
       "5              None              None           None                  None   \n",
       "6              None              None           None                  None   \n",
       "7              None              None           None                  None   \n",
       "8              None              None           None                  None   \n",
       "9              None              None           None                  None   \n",
       "\n",
       "  first_issue_created_at first_pr_created_at last_issue_created_at  \\\n",
       "0                   None                None                  None   \n",
       "1                   None                None                  None   \n",
       "2                   None                None                  None   \n",
       "3                   None                None                  None   \n",
       "4                   None                None                  None   \n",
       "5                   None                None                  None   \n",
       "6                   None                None                  None   \n",
       "7                   None                None                  None   \n",
       "8                   None                None                  None   \n",
       "9                   None                None                  None   \n",
       "\n",
       "  last_pr_created_at last_pr_merged_at  num_fetches_last_30_days  \\\n",
       "0               None              None                       NaN   \n",
       "1               None              None                       NaN   \n",
       "2               None              None                       1.0   \n",
       "3               None              None                       NaN   \n",
       "4               None              None                       NaN   \n",
       "5               None              None                       1.0   \n",
       "6               None              None                       1.0   \n",
       "7               None              None                       2.0   \n",
       "8               None              None                       1.0   \n",
       "9               None              None                       NaN   \n",
       "\n",
       "  fetchers_last_30_days  num_fetchers_last_30_days            first_push_at  \\\n",
       "0                  None                        NaN  2015-08-03 02:18:48.000   \n",
       "1                  None                        NaN  2015-08-03 02:18:18.000   \n",
       "2            [12613545]                        1.0  2015-08-03 02:17:01.000   \n",
       "3                  None                        NaN  2015-08-03 02:16:10.000   \n",
       "4                  None                        NaN  2015-08-03 02:15:10.000   \n",
       "5            [12613545]                        1.0  2015-08-03 02:12:42.000   \n",
       "6            [12613545]                        1.0  2015-08-03 02:11:22.000   \n",
       "7            [12613545]                        1.0  2015-08-03 02:10:33.000   \n",
       "8                [None]                        1.0  2017-03-28 04:12:57.000   \n",
       "9                  None                        NaN                     None   \n",
       "\n",
       "              last_push_at  num_pushes_last_30_days num_commits_last_30_days  \\\n",
       "0  2015-08-03 02:18:48.000                      0.0                     None   \n",
       "1  2015-08-03 02:18:18.000                      0.0                     None   \n",
       "2  2015-08-03 02:17:01.000                      0.0                     None   \n",
       "3  2015-08-03 02:16:10.000                      0.0                     None   \n",
       "4  2015-08-03 02:15:10.000                      0.0                     None   \n",
       "5  2015-08-03 02:12:42.000                      0.0                     None   \n",
       "6  2015-08-03 02:11:22.000                      0.0                     None   \n",
       "7  2015-08-03 02:10:33.000                      0.0                     None   \n",
       "8  2017-03-28 04:14:10.000                      0.0                     None   \n",
       "9                     None                      NaN                     None   \n",
       "\n",
       "  committers_last_30_days num_committers_last_30_days  num_views_last_30_days  \\\n",
       "0                    None                        None                     NaN   \n",
       "1                    None                        None                     NaN   \n",
       "2                    None                        None                     NaN   \n",
       "3                    None                        None                     NaN   \n",
       "4                    None                        None                     NaN   \n",
       "5                    None                        None                     NaN   \n",
       "6                    None                        None                     NaN   \n",
       "7                    None                        None                     NaN   \n",
       "8                    None                        None                     8.0   \n",
       "9                    None                        None                     1.0   \n",
       "\n",
       "  logged_in_viewers_last_30_days  num_logged_in_viewers_last_30_days  \\\n",
       "0                           None                                 NaN   \n",
       "1                           None                                 NaN   \n",
       "2                           None                                 NaN   \n",
       "3                           None                                 NaN   \n",
       "4                           None                                 NaN   \n",
       "5                           None                                 NaN   \n",
       "6                           None                                 NaN   \n",
       "7                           None                                 NaN   \n",
       "8               [None, 34088524]                                 2.0   \n",
       "9                     [32748793]                                 1.0   \n",
       "\n",
       "  pct_threads_with_assignee num_protected_branches protected_branch_names  \n",
       "0                      None                   None                   None  \n",
       "1                      None                   None                   None  \n",
       "2                      None                   None                   None  \n",
       "3                      None                   None                   None  \n",
       "4                      None                   None                   None  \n",
       "5                      None                   None                   None  \n",
       "6                      None                   None                   None  \n",
       "7                      None                   None                   None  \n",
       "8                      None                   None                   None  \n",
       "9                      None                   None                   None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "select * from hive.entities.repos limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo_homepage</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>issue_number</th>\n",
       "      <th>body</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>issue_title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codeformatter</td>\n",
       "      <td>28896487</td>\n",
       "      <td></td>\n",
       "      <td>68794631</td>\n",
       "      <td>112</td>\n",
       "      <td>some checks that we will soon implement are ra...</td>\n",
       "      <td>add a switch to optout of slow checks</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SalesforceMobileSDK-iOS</td>\n",
       "      <td>2338086</td>\n",
       "      <td>https://github.com/forcedotcom/SalesforceMobil...</td>\n",
       "      <td>68794668</td>\n",
       "      <td>992</td>\n",
       "      <td>this fixes the issues we were seeing in ios  a...</td>\n",
       "      <td>taking the broken completion block hack out of...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KOS</td>\n",
       "      <td>17217530</td>\n",
       "      <td>None</td>\n",
       "      <td>68794714</td>\n",
       "      <td>831</td>\n",
       "      <td>since its not possible to tell the current sta...</td>\n",
       "      <td>enhancement a state field in the core right cl...</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMS</td>\n",
       "      <td>9184554</td>\n",
       "      <td>http://www.kooboo.com</td>\n",
       "      <td>17981461</td>\n",
       "      <td>122</td>\n",
       "      <td>for examplesome one login in httpwwwkooboocomc...</td>\n",
       "      <td>oauth login not work correctly in a site with ...</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ipython</td>\n",
       "      <td>658518</td>\n",
       "      <td>https://ipython.readthedocs.org</td>\n",
       "      <td>17981488</td>\n",
       "      <td>4005</td>\n",
       "      <td>according to the documentation it should launc...</td>\n",
       "      <td>ipythonstart_kernel doesnt work</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 repo_name   repo_id  \\\n",
       "0            codeformatter  28896487   \n",
       "1  SalesforceMobileSDK-iOS   2338086   \n",
       "2                      KOS  17217530   \n",
       "3                      CMS   9184554   \n",
       "4                  ipython    658518   \n",
       "\n",
       "                                       repo_homepage  issue_id  issue_number  \\\n",
       "0                                                     68794631           112   \n",
       "1  https://github.com/forcedotcom/SalesforceMobil...  68794668           992   \n",
       "2                                               None  68794714           831   \n",
       "3                              http://www.kooboo.com  17981461           122   \n",
       "4                    https://ipython.readthedocs.org  17981488          4005   \n",
       "\n",
       "                                                body  \\\n",
       "0  some checks that we will soon implement are ra...   \n",
       "1  this fixes the issues we were seeing in ios  a...   \n",
       "2  since its not possible to tell the current sta...   \n",
       "3  for examplesome one login in httpwwwkooboocomc...   \n",
       "4  according to the documentation it should launc...   \n",
       "\n",
       "                                         issue_title  issue_title_len  \\\n",
       "0              add a switch to optout of slow checks                8   \n",
       "1  taking the broken completion block hack out of...                9   \n",
       "2  enhancement a state field in the core right cl...               10   \n",
       "3  oauth login not work correctly in a site with ...               12   \n",
       "4                    ipythonstart_kernel doesnt work                3   \n",
       "\n",
       "   body_len  \n",
       "0        45  \n",
       "1        14  \n",
       "2        74  \n",
       "3        26  \n",
       "4       125  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from time import sleep\n",
    "dask_df = dd.from_pandas(data=df, chunksize=10000)\n",
    "dask_df = dask_df.repartition(npartitions=8)\n",
    "dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from DeepText.preprocess import preProcessor\n",
    "pp = preProcessor()\n",
    "clean_func = pp.default_cleaner\n",
    "tokenizer = pp.default_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Check to see if dask is actually working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/dask/dataframe/core.py:1979: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 16 ms, total: 92 ms\n",
      "Wall time: 1.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dask_df.body.apply(lambda x: sleep(.0001)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 544 ms, sys: 132 ms, total: 676 ms\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.body.apply(lambda x: sleep(.0001)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 7.81 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/dask/dataframe/core.py:1979: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dask_df['new_col'] = dask_df.body.apply(clean_func)\n",
    "dask_df['token_body'] = dask_df.new_col.apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 12 ms, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125    [hi, there, \\n , thanks, again, for, making, t...\n",
       "126    [hi, \\n, considering, mock, objects, which, ar...\n",
       "127    [see, the, thread, starting, here, \\n, httplis...\n",
       "128    [phoenix, httpphoenixapacheorg, is, an, open, ...\n",
       "129    [pr, for, issue, httpsgithubcomflywayflywayiss...\n",
       "130    [i, do, nt, believe, nslookup, on, its, own, w...\n",
       "131    [this, change, was, added, in, then, removed, ...\n",
       "132    [if, a, stream, output, is, missing, text, log...\n",
       "133    [previously, the, geoip, plugin, would, crash,...\n",
       "134    [since, it, shows, regular, tracebacks, now, r...\n",
       "135    [we, would, like, to, be, able, to, retrieve, ...\n",
       "136    [httpsgithubcomjamesmartininline_svg, \\n, i, m...\n",
       "137    [i, have, just, install, the, parsimony, cms, ...\n",
       "138    [odoo, have, bug, with, webclient, view, on, c...\n",
       "139    [i, would, like, to, access, the, machinename,...\n",
       "140    [solution, \\n , add, it, to, makefileam, \\n, n...\n",
       "141    [explain, the, issue, \\n, the, alignment, of, ...\n",
       "142    [hello, wave, \\n, rocketrocketrocket, \\n, gulp...\n",
       "143    [they, were, nt, ready, to, be, enabled, at, b...\n",
       "144    [source, suggestion, ajax, url, pathsuggestion...\n",
       "145    [most, tests, are, working, fine, there, is, o...\n",
       "146    [github, is, showing, that, our, tests, are, f...\n",
       "147    [hey, there, \\n, when, i, tap, the, mask, exam...\n",
       "148    [hello, everybody, i, m, using, rvm, and, can,...\n",
       "149    [terminal, output, \\n, info, createhierarchycp...\n",
       "150    [process, sample, \\n, sampling, process, for, ...\n",
       "151    [if, a, buttons, class, is, set, to, start, an...\n",
       "152    [maybe, this, can, be, done, already, but, i, ...\n",
       "153    [while, playing, around, with, substance, i, g...\n",
       "154    [after, scaling, a, circle, and, cloning, it, ...\n",
       "155    [it, seems, i, ca, nt, launch, osrmrouted, on,...\n",
       "Name: token_body, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dask_df.token_body.loc[125:155].compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
