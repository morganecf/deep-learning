{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n== Background ==\\n\\nIntroduced by Ian Goodfellow in University of Montreal!\\nYann LeCunn called them the most important deep learning development of past 10y.\\n\\n== Intuition ==\\n\\nGAN == 2 networks in a forger-police relationship\\nG == forger = generator\\nD == police == discriminator\\n\\nD learns a conditional distribution P(Y|X), or a function y = f(x) that maps inputs to outputs.\\nG learns a JOINT distribution of inputs and outputs: P(X, Y). Can use Bayes rule to convert to P(Y|X),\\nbut also can be used to create new samples (x, y).\\n\\nG generates images to try to trick D into believing they're real (from the training set).\\nD looks at an image and estimates if it's fake or real. G starts off generating\\nrandom noise into images.\\n\\nIf the training set is large enough, D (police) cannot just memorize the training data.\\nRegularization strategies also help with this. So it must learn a generalizing\\nfunction: rules that govern the look of the images, while G learns to create\\nnew images that look like ones from the training set in order to trick D.\\n\\nD & G play a game: show D a mixed batch of real images from training and fake\\nimages from G. Optimize D to say NO to fake images and YES to real images.\\nOptimimze G to fool D into believing fake were real.\\n\\nIn mathematical terms: minimize the classification error w/r/t D and maximize\\nit w/r/t G.\\n\\n\\n== Laplacian Pyramid Technique ==\\n\\nTrain on images of increasing size. Start off with normal sized images:\\n64x64. Convert them to 8x8. Start training on these, then in each step\\nincrease their size: 8x8 --> 16x16 --> 32x32 --> 64x64.\\n\\nIn each step, another G and D are trained, and they're trained to learn good\\nrefinements of the upscaled, blurry images. D gets fed refined/sharpened images\\nand needs to tell if they're real (blurry from training set w/ refinements) or\\nfake (blurry but refinement was done by G).\\n\\nSo G ends up learning how to generate good refinements.\\nD learns what a good refined image looks like.\\n\\n== Architecture ==\\n\\nG == full laplacian pyramid in one network. its layers scale the images up.\\nD == convolutional network with multiple branches. Different branches are\\nsupposed to focus on different regions of the image, last branch analyzes image in its totality.\\n\\n\\n== Tricks ==\\n\\nTo avoid producing gibberish noise! Make sure G and D don't become too strong compared to the other\\nDon't let D win and classify all images correctly  - G won't be able to learn from it.\\nIf G wins, it is usually by exploiting a meaningless weakness in the image (ex: coloring the entire image blue).\\n\\nKeep track of:\\n1. How good G is at fooling D\\n2. How good D is at classifying fake as fakes (TN)\\n3. How good D is at classifying real as real (TP)\\n\\nIf one of the networks is too good, skip updating its params:\\nmargin // user-defined margin\\n\\nif err_F < margin or err_R < margin:\\n    D.optimize = false\\n\\nif err_F > 1 - margin or err_R > 1 - magin:\\n    G.optimize = false\\n\\nif !G.optimize and !D.optimize:\\n    G.optimize = true\\n    D.optimize = true\\n\\n\\nAnother trick: if G is performing poorly, try to regularize (penalize) D.\\nIncrement D's L2 penalty if G isn't within a certain target range.\\nIf G fools D 50% of the time, err would be log(0.5) = 0.69. Set target\\nrange to [0.9 - 1.2] so that D is better than G but not by too much.\\n\\nOther tricks:\\n- Batch normalization in G but NOT in D (that will make D too powerful)*\\n- Dropout (especially in D)**\\n- Decrease # of features of D such that G has more params\\n- In first few epochs it's normal if it generates nonsense, but probably \\n  bad news if it generates equal or nearly equal images.\\n\\n\\n== Resources ==\\n\\n- Cat generator in lua torch: https://github.com/aleju/cat-generator\\n- Face generator in lua torch: http://torch.ch/blog/2015/11/13/gan.html\\n    - Includes tricks & small explanation about variational autoencoders\\n- http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "== Background ==\n",
    "\n",
    "Introduced by Ian Goodfellow in University of Montreal!\n",
    "Yann LeCunn called them the most important deep learning development of past 10y.\n",
    "\n",
    "== Intuition ==\n",
    "\n",
    "GAN == 2 networks in a forger-police relationship\n",
    "G == forger = generator\n",
    "D == police == discriminator\n",
    "\n",
    "D learns a conditional distribution P(Y|X), or a function y = f(x) that maps inputs to outputs.\n",
    "G learns a JOINT distribution of inputs and outputs: P(X, Y). Can use Bayes rule to convert to P(Y|X),\n",
    "but also can be used to create new samples (x, y).\n",
    "\n",
    "G generates images to try to trick D into believing they're real (from the training set).\n",
    "D looks at an image and estimates if it's fake or real. G starts off generating\n",
    "random noise into images.\n",
    "\n",
    "If the training set is large enough, D (police) cannot just memorize the training data.\n",
    "Regularization strategies also help with this. So it must learn a generalizing\n",
    "function: rules that govern the look of the images, while G learns to create\n",
    "new images that look like ones from the training set in order to trick D.\n",
    "\n",
    "D & G play a game: show D a mixed batch of real images from training and fake\n",
    "images from G. Optimize D to say NO to fake images and YES to real images.\n",
    "Optimimze G to fool D into believing fake were real.\n",
    "\n",
    "In mathematical terms: minimize the classification error w/r/t D and maximize\n",
    "it w/r/t G. Or minimizing the f-divergence by (difference in probability distributions). \n",
    "\n",
    "Converges when D is no longer able to tell real from fake images.\n",
    "\n",
    "\n",
    "== Laplacian Pyramid Technique ==\n",
    "\n",
    "Train on images of increasing size. Start off with normal sized images:\n",
    "64x64. Convert them to 8x8. Start training on these, then in each step\n",
    "increase their size: 8x8 --> 16x16 --> 32x32 --> 64x64.\n",
    "\n",
    "In each step, another G and D are trained, and they're trained to learn good\n",
    "refinements of the upscaled, blurry images. D gets fed refined/sharpened images\n",
    "and needs to tell if they're real (blurry from training set w/ refinements) or\n",
    "fake (blurry but refinement was done by G).\n",
    "\n",
    "So G ends up learning how to generate good refinements.\n",
    "D learns what a good refined image looks like.\n",
    "\n",
    "== Architecture ==\n",
    "\n",
    "G == full laplacian pyramid in one network. its layers scale the images up.\n",
    "D == convolutional network with multiple branches. Different branches are\n",
    "supposed to focus on different regions of the image, last branch analyzes image in its totality.\n",
    "\n",
    "\n",
    "== Tricks ==\n",
    "\n",
    "To avoid producing gibberish noise! Make sure G and D don't become too strong compared to the other\n",
    "Don't let D win and classify all images correctly  - G won't be able to learn from it.\n",
    "If G wins, it is usually by exploiting a meaningless weakness in the image (ex: coloring the entire image blue).\n",
    "\n",
    "Keep track of:\n",
    "1. How good G is at fooling D\n",
    "2. How good D is at classifying fake as fakes (TN)\n",
    "3. How good D is at classifying real as real (TP)\n",
    "\n",
    "If one of the networks is too good, skip updating its params:\n",
    "margin // user-defined margin\n",
    "\n",
    "if err_F < margin or err_R < margin:\n",
    "    D.optimize = false\n",
    "\n",
    "if err_F > 1 - margin or err_R > 1 - magin:\n",
    "    G.optimize = false\n",
    "\n",
    "if !G.optimize and !D.optimize:\n",
    "    G.optimize = true\n",
    "    D.optimize = true\n",
    "\n",
    "\n",
    "Another trick: if G is performing poorly, try to regularize (penalize) D.\n",
    "Increment D's L2 penalty if G isn't within a certain target range.\n",
    "If G fools D 50% of the time, err would be log(0.5) = 0.69. Set target\n",
    "range to [0.9 - 1.2] so that D is better than G but not by too much.\n",
    "\n",
    "GAN generator takes uniform(-1, 1) distrib as input image, so make sure this\n",
    "is what's being input when creating test images (after training).\n",
    "\n",
    "Other tricks:\n",
    "- Batch normalization in G but NOT in D (that will make D too powerful)*\n",
    "- Dropout (especially in D)**\n",
    "- Decrease # of features of D such that G has more params\n",
    "- In first few epochs it's normal if it generates nonsense, but probably \n",
    "  bad news if it generates equal or nearly equal images.\n",
    "\n",
    "\n",
    "== Resources ==\n",
    "\n",
    "- Cat generator in lua torch: https://github.com/aleju/cat-generator\n",
    "- Face generator in lua torch: http://torch.ch/blog/2015/11/13/gan.html\n",
    "    - Includes tricks & small explanation about variational autoencoders\n",
    "- Code from: http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\n",
    "- http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html\n",
    "- \"REAL\" code used for sigilizer: https://github.com/carpedm20/DCGAN-tensorflow\n",
    "\n",
    "\n",
    "== Wasserstein GANs ==\n",
    "\n",
    "Paper: https://arxiv.org/abs/1701.07875\n",
    "\n",
    "GANs are notoriously unstable. WGANs attempt to improve upon stability. \n",
    "\n",
    "Normal GAN uses softplus (nonlinearity): applied to last matrix multiplication when computing loss.\n",
    "(Decision boundary is a nonlinearity?) With WGANs, they take the output of the last matrix multiplication\n",
    "directly, and restrict the range of weights to guarantee Lipschitz continuity (a strong uniform continuity\n",
    "-- limited in how fast it can change-- for every pair of points on graph, magnitude of slope cannot get\n",
    "greater than a certain constant). So they regularize/squeeze the weights. Guarantees that the discriminator\n",
    "is continuous w/r/t its parameters and therefore is continuously differentiable at every point -- this means\n",
    "metric becomes more interpretable. Having a continuous/differentiable metric means can strongly train the\n",
    "discriminator before doing an update to the generator, to get more reliable gradients ... (with regular\n",
    "GANs, a strong D means vanishing gradients(?)).\n",
    "\n",
    "Basically seems to allow you to make the discriminator stronger without imperiling the generator's loss.\n",
    "\n",
    "Changes:\n",
    "- D doesn't produce sigmoid/probabilistic output\n",
    "    - instead, loss in D == y - y_hat -- just the diff between real/fake images\n",
    "- train D multiple times for each G update\n",
    "- clamp weights in D to ~0\n",
    "- low learning rate\n",
    "- optimizers do not need momentum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll learn to approximate a gaussian distribution\n",
    "\n",
    "# This is a real gaussian with mean 4 and variance 0.5\n",
    "# This is the training data\n",
    "class DataDistribution(object):\n",
    "    def __init__(self):\n",
    "        self.mu = 4\n",
    "        self.sigma = 0.5\n",
    "\n",
    "    def sample(self, N):\n",
    "        samples = np.random.normal(self.mu, self.sigma, N)\n",
    "        samples.sort()\n",
    "        return samples\n",
    "\n",
    "# Generator initially generates random noise within a certain given range\n",
    "class GeneratorDistribution(object):\n",
    "    def __init__(self, range):\n",
    "        self.range = range\n",
    "\n",
    "    def sample(self, N):\n",
    "        return np.linspace(-self.range, self.range, N) + \\\n",
    "            np.random.random(N) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input, output_dim, scope='linear', stddev=1.0):\n",
    "    # returns a context manager that makes sure variables\n",
    "    # are from the same graph. get_variable gets variables\n",
    "    # belonging to the scope of the \"with\" context\n",
    "    with tf.variable_scope(scope):\n",
    "        # hidden layer weights -- random normal\n",
    "        w = tf.get_variable(\n",
    "            'w',\n",
    "            [input.get_shape()[1], output_dim],\n",
    "            initializer=tf.random_normal_initializer(stddev=stddev)\n",
    "        )\n",
    "        # constant\n",
    "        b = tf.get_variable(\n",
    "            'b',\n",
    "            [output_dim],\n",
    "            initializer=tf.constant_initializer(0.0)\n",
    "        )\n",
    "        # matrix multiplication -- x * w + b\n",
    "        return tf.matmul(input, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator G is a linear transformation passed through\n",
    "# nonlinearity -- softplus function. For an image, would\n",
    "# be multiplying image by weights, adding constant, transforming\n",
    "# image through non-linear fn, then adding another constant.\n",
    "# softplus is simply: log(exp(X) + 1) == log(X^e + 1)\n",
    "def generator(input, h_dim):\n",
    "    # make predictions - x * w + constant\n",
    "    pred = linear(input, h_dim, 'g0')\n",
    "    # hidden layer weights\n",
    "    h0 = tf.nn.softplus(pred)\n",
    "    # hidden layer weights -- non_linearity(x * w + b) * 1 + constant\n",
    "    h1 = linear(h0, 1, 'g1')\n",
    "    return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D has more hidden layers because it needs to be more powerful\n",
    "# than G -- for this type of problem, bad at discriminating between\n",
    "# real and fake. Uses tanh and sigmoid.\n",
    "def discriminator_nobatch(input, hidden_size):\n",
    "    h0 = tf.tanh(linear(input, hidden_size * 2, 'd0'))\n",
    "    h1 = tf.tanh(linear(h0, hidden_size * 2, 'd1'))\n",
    "    h2 = tf.tanh(linear(h1, hidden_size * 2, 'd2'))\n",
    "    h3 = tf.sigmoid(linear(h2, 1, 'd3'))\n",
    "    return h3\n",
    "\n",
    "def discriminator(input, h_dim, minibatch_layer=True):\n",
    "    # relu is a type of activation function: https://www.tensorflow.org/api_guides/python/nn#Activation_Functions\n",
    "    # these are all nonlinearities, like sigmoid, that can make classification decision\n",
    "    h0 = tf.nn.relu(linear(input, h_dim * 2, 'd0'))\n",
    "    h1 = tf.nn.relu(linear(h0, h_dim * 2, 'd1'))\n",
    "\n",
    "    # without the minibatch layer, the discriminator needs an additional layer\n",
    "    # to have enough capacity to separate the two distributions correctly\n",
    "    if minibatch_layer:\n",
    "        # use minibatch on an intermediate layer\n",
    "        h2 = minibatch(h1)\n",
    "    else:\n",
    "        h2 = tf.nn.relu(linear(h1, h_dim * 2, scope='d2'))\n",
    "\n",
    "    # Final decision\n",
    "    h3 = tf.sigmoid(linear(h2, 1, scope='d3'))\n",
    "    return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One of the main failure modes of GANs is their tendency to collapse to a small\n",
    "# range of points. This can be circumvented by allowing D to look at multiple points\n",
    "# at once: minibatch discrimination == any method where D looks at an entire batch\n",
    "# of samples to determine if they are real or fake. One type of algorithm that does\n",
    "# this looks at a sample and then that sample's distance from all other samples in the\n",
    "# batch. Distance measures are then combined with the input (as an extra feature) and\n",
    "# D can use this information while making a decision.\n",
    "# more info here: http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\n",
    "def minibatch(input, num_kernels=5, kernel_dim=3):\n",
    "    # input is an intermediate layer. multiply it by 3D tensor to produce a matrix\n",
    "    # of size num_kernels * kernel_dim\n",
    "    x = linear(input, num_kernels * kernel_dim, scope='minibatch', stddev=0.02)\n",
    "    activation = tf.reshape(x, (-1, num_kernels, kernel_dim))\n",
    "    diffs = tf.expand_dims(activation, 3) - \\\n",
    "        tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\n",
    "    abs_diffs = tf.reduce_sum(tf.abs(diffs), 2)\n",
    "    minibatch_features = tf.reduce_sum(tf.exp(-abs_diffs), 2)\n",
    "    return tf.concat([input, minibatch_features], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Adam optimizer\n",
    "def optimizer(loss, var_list):\n",
    "    learning_rate = 0.001\n",
    "    step = tf.Variable(0, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "        loss,\n",
    "        global_step=step,\n",
    "        var_list=var_list\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes discriminiator outputs can reach values close to\n",
    "# (or even slightly less than) zero due to numerical rounding.\n",
    "# This just makes sure that we exclude those values so that we don't\n",
    "# end up with NaNs during optimization.\n",
    "def log(x):\n",
    "    return tf.log(tf.maximum(x, 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the GAN!\n",
    "class GAN(object):\n",
    "    def __init__(self, params):\n",
    "        # This defines the generator network - it takes samples from a noise\n",
    "        # distribution as input, and passes them through an MLP.\n",
    "        with tf.variable_scope('G'):\n",
    "            self.z = tf.placeholder(tf.float32, shape=(params.batch_size, 1))\n",
    "            self.G = generator(self.z, params.hidden_size)\n",
    "\n",
    "        # The discriminator tries to tell the difference between samples from\n",
    "        # the true data distribution (self.x) and the generated samples\n",
    "        # (self.z).\n",
    "        #\n",
    "        # Here we create two copies of the discriminator network\n",
    "        # that share parameters, as you cannot use the same network with\n",
    "        # different inputs in TensorFlow. One copy of D processes x  and\n",
    "        # the other processes the generated images G(z).\n",
    "        \n",
    "        # Input of D1 is a single sample of the real data. Input of D2\n",
    "        # is a single sample of the fake data. So when optimizing D,\n",
    "        # we want to maximize D1 and minimize D2.\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=(params.batch_size, 1))\n",
    "        with tf.variable_scope('D'):\n",
    "            self.D1 = discriminator(\n",
    "                self.x,\n",
    "                params.hidden_size,\n",
    "                params.minibatch\n",
    "            )\n",
    "        with tf.variable_scope('D', reuse=True):\n",
    "            self.D2 = discriminator(\n",
    "                self.G,\n",
    "                params.hidden_size,\n",
    "                params.minibatch\n",
    "            )\n",
    "\n",
    "        # Define the loss for discriminator and generator networks\n",
    "        # (see the original paper for details), and create optimizers for both\n",
    "        # For discriminator:\n",
    "            # min(-log(D1)) ==> maximize D1\n",
    "            # min(-log(1 - D2)) ==> minimize D2\n",
    "        # For generator:\n",
    "            # min(-log(D2)) ==> maximize D2\n",
    "        self.loss_d = tf.reduce_mean(-log(self.D1) - log(1 - self.D2))\n",
    "        self.loss_g = tf.reduce_mean(-log(self.D2))\n",
    "\n",
    "        vars = tf.trainable_variables()\n",
    "        self.d_params = [v for v in vars if v.name.startswith('D/')]\n",
    "        self.g_params = [v for v in vars if v.name.startswith('G/')]\n",
    "\n",
    "        self.opt_d = optimizer(self.loss_d, self.d_params)\n",
    "        self.opt_g = optimizer(self.loss_g, self.g_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, gen, params):\n",
    "    anim_frames = []\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        tf.local_variables_initializer().run()\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for step in range(params.num_steps + 1):\n",
    "            ## Update discriminator\n",
    "            \n",
    "            # Get real data\n",
    "            x = data.sample(params.batch_size)\n",
    "            # Get fake data\n",
    "            z = gen.sample(params.batch_size)\n",
    "            # Compute loss\n",
    "            loss_d, _, = session.run([model.loss_d, model.opt_d], {\n",
    "                model.x: np.reshape(x, (params.batch_size, 1)),\n",
    "                model.z: np.reshape(z, (params.batch_size, 1))\n",
    "            })\n",
    "\n",
    "            ## Update generator\n",
    "            \n",
    "            # Get fake data -- why isn't it using z from above?\n",
    "            z = gen.sample(params.batch_size)\n",
    "            loss_g, _ = session.run([model.loss_g, model.opt_g], {\n",
    "                model.z: np.reshape(z, (params.batch_size, 1))\n",
    "            })\n",
    "\n",
    "            if step % params.log_every == 0:\n",
    "                print('{}: {:.4f}\\t{:.4f}'.format(step, loss_d, loss_g))\n",
    "\n",
    "            if params.anim_path and (step % params.anim_every == 0):\n",
    "                anim_frames.append(\n",
    "                    samples(model, session, data, gen.range, params.batch_size)\n",
    "                )\n",
    "\n",
    "        if params.anim_path:\n",
    "            save_animation(anim_frames, params.anim_path, gen.range)\n",
    "        else:\n",
    "            samps = samples(model, session, data, gen.range, params.batch_size)\n",
    "            plot_distributions(samps, gen.range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def samples(model, session, data, sample_range, batch_size, num_points=10000, num_bins=100):\n",
    "    '''\n",
    "    Return a tuple (db, pd, pg), where db is the current decision\n",
    "    boundary, pd is a histogram of samples from the data distribution,\n",
    "    and pg is a histogram of generated samples.\n",
    "    '''\n",
    "    xs = np.linspace(-sample_range, sample_range, num_points)\n",
    "    bins = np.linspace(-sample_range, sample_range, num_bins)\n",
    "\n",
    "    # decision boundary\n",
    "    db = np.zeros((num_points, 1))\n",
    "    for i in range(num_points // batch_size):\n",
    "        db[batch_size * i:batch_size * (i + 1)] = session.run(\n",
    "            model.D1,\n",
    "            {\n",
    "                model.x: np.reshape(\n",
    "                    xs[batch_size * i:batch_size * (i + 1)],\n",
    "                    (batch_size, 1)\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # data distribution\n",
    "    d = data.sample(num_points)\n",
    "    pd, _ = np.histogram(d, bins=bins, density=True)\n",
    "\n",
    "    # generated samples\n",
    "    zs = np.linspace(-sample_range, sample_range, num_points)\n",
    "    g = np.zeros((num_points, 1))\n",
    "    for i in range(num_points // batch_size):\n",
    "        g[batch_size * i:batch_size * (i + 1)] = session.run(\n",
    "            model.G,\n",
    "            {\n",
    "                model.z: np.reshape(\n",
    "                    zs[batch_size * i:batch_size * (i + 1)],\n",
    "                    (batch_size, 1)\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    pg, _ = np.histogram(g, bins=bins, density=True)\n",
    "\n",
    "    return db, pd, pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
