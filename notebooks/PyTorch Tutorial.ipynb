{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can technically create neural networks using numpy, which exposes\n",
    "# an n-dimensional array object and lots of optimized functions for\n",
    "# manipulating these arrays. But they also know nothing about graphs or\n",
    "# deep learning or gradients. But here's an example of fitting a 2-layer\n",
    "# netowrk to random data, implementing forward and backwards passes, to\n",
    "# get train a neural network to predict y from x.\n",
    "\n",
    "# Neural networks are tasked with learning an appropriate\n",
    "# INTERNAL representation so that it can take arbitrary\n",
    "# data and map it to the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the batch size (number of datapoints)\n",
    "N = 64\n",
    "\n",
    "# the input \"feature\" dimension / layer\n",
    "D_in = 1000\n",
    "\n",
    "# the hidden dimension (# of nodes in the hidden layer)\n",
    "H = 100\n",
    "\n",
    "# output dimension / layer\n",
    "D_out = 10\n",
    "\n",
    "# Create random input/output data\n",
    "x = np.random.randn(N, D_in)      # 64 x 1000\n",
    "y = np.random.randn(N, D_out)     # 64 x 10\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)     # 1000 x 100 - input weights\n",
    "w2 = np.random.randn(H, D_out)    # 100 x 10 - hidden layer weights\n",
    "\n",
    "learning_rate = 1e-6\n",
    "num_passes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0 41778463.3908\n",
      "loss: 1 40572998.7816\n",
      "loss: 2 38626629.3182\n",
      "loss: 3 30137349.5522\n",
      "loss: 4 18679640.5011\n",
      "loss: 5 9576483.20226\n",
      "loss: 6 4799231.20306\n",
      "loss: 7 2671127.17678\n",
      "loss: 8 1738119.51262\n",
      "loss: 9 1276556.33142\n",
      "loss: 10 1007632.17666\n",
      "loss: 11 826406.804325\n",
      "loss: 12 692319.845938\n",
      "loss: 13 587738.778106\n",
      "loss: 14 503601.869539\n",
      "loss: 15 434655.826133\n",
      "loss: 16 377497.622608\n",
      "loss: 17 329639.652659\n",
      "loss: 18 289289.501504\n",
      "loss: 19 255028.537896\n",
      "loss: 20 225766.350413\n",
      "loss: 21 200623.09997\n",
      "loss: 22 178933.258419\n",
      "loss: 23 160128.024002\n",
      "loss: 24 143739.349369\n",
      "loss: 25 129381.822232\n",
      "loss: 26 116759.035368\n",
      "loss: 27 105623.534134\n",
      "loss: 28 95765.140964\n",
      "loss: 29 87006.2688523\n",
      "loss: 30 79204.0645084\n",
      "loss: 31 72233.1821844\n",
      "loss: 32 65990.4926418\n",
      "loss: 33 60384.4886444\n",
      "loss: 34 55346.170914\n",
      "loss: 35 50800.3177728\n",
      "loss: 36 46690.3759852\n",
      "loss: 37 42967.392981\n",
      "loss: 38 39586.7819258\n",
      "loss: 39 36511.2411272\n",
      "loss: 40 33710.1194788\n",
      "loss: 41 31156.6210181\n",
      "loss: 42 28822.5415483\n",
      "loss: 43 26688.7897666\n",
      "loss: 44 24735.475391\n",
      "loss: 45 22944.4397834\n",
      "loss: 46 21299.9497634\n",
      "loss: 47 19787.3524856\n",
      "loss: 48 18394.8653543\n",
      "loss: 49 17111.6777988\n",
      "loss: 50 15928.5524432\n",
      "loss: 51 14835.9357171\n",
      "loss: 52 13825.8860482\n",
      "loss: 53 12891.8414771\n",
      "loss: 54 12027.1740736\n",
      "loss: 55 11226.2729442\n",
      "loss: 56 10483.6389522\n",
      "loss: 57 9794.71141825\n",
      "loss: 58 9155.22581589\n",
      "loss: 59 8561.27683162\n",
      "loss: 60 8008.94945216\n",
      "loss: 61 7495.3915545\n",
      "loss: 62 7017.33665332\n",
      "loss: 63 6572.5532596\n",
      "loss: 64 6158.14278106\n",
      "loss: 65 5771.76560875\n",
      "loss: 66 5411.40248836\n",
      "loss: 67 5075.80506376\n",
      "loss: 68 4762.73636205\n",
      "loss: 69 4470.7352947\n",
      "loss: 70 4198.01445393\n",
      "loss: 71 3942.96020344\n",
      "loss: 72 3704.40186473\n",
      "loss: 73 3481.19032769\n",
      "loss: 74 3272.26668446\n",
      "loss: 75 3076.82045043\n",
      "loss: 76 2893.84158535\n",
      "loss: 77 2723.1290334\n",
      "loss: 78 2563.02443021\n",
      "loss: 79 2412.91843137\n",
      "loss: 80 2272.1058498\n",
      "loss: 81 2139.9464113\n",
      "loss: 82 2015.99418787\n",
      "loss: 83 1899.59428024\n",
      "loss: 84 1790.17408744\n",
      "loss: 85 1687.37937879\n",
      "loss: 86 1590.78442275\n",
      "loss: 87 1499.94876205\n",
      "loss: 88 1414.53879193\n",
      "loss: 89 1334.2109782\n",
      "loss: 90 1258.63575167\n",
      "loss: 91 1187.51628642\n",
      "loss: 92 1120.57415846\n",
      "loss: 93 1057.57378733\n",
      "loss: 94 998.264550209\n",
      "loss: 95 942.390581162\n",
      "loss: 96 889.769190802\n",
      "loss: 97 840.186794687\n",
      "loss: 98 793.483188284\n",
      "loss: 99 749.453527345\n",
      "loss: 100 707.953059721\n",
      "loss: 101 668.825270653\n",
      "loss: 102 631.936618537\n",
      "loss: 103 597.144957219\n",
      "loss: 104 564.320413536\n",
      "loss: 105 533.364839278\n",
      "loss: 106 504.161236666\n",
      "loss: 107 476.601159081\n",
      "loss: 108 450.592134353\n",
      "loss: 109 426.035069119\n",
      "loss: 110 402.863407578\n",
      "loss: 111 380.980446355\n",
      "loss: 112 360.319392097\n",
      "loss: 113 340.806783472\n",
      "loss: 114 322.378573927\n",
      "loss: 115 304.978397756\n",
      "loss: 116 288.539278059\n",
      "loss: 117 273.016674639\n",
      "loss: 118 258.348169099\n",
      "loss: 119 244.486603381\n",
      "loss: 120 231.381465402\n",
      "loss: 121 218.997463513\n",
      "loss: 122 207.292418973\n",
      "loss: 123 196.224871293\n",
      "loss: 124 185.762895896\n",
      "loss: 125 175.86932819\n",
      "loss: 126 166.517258793\n",
      "loss: 127 157.670522249\n",
      "loss: 128 149.305058485\n",
      "loss: 129 141.391477793\n",
      "loss: 130 133.905787503\n",
      "loss: 131 126.826711452\n",
      "loss: 132 120.12847963\n",
      "loss: 133 113.790332773\n",
      "loss: 134 107.792915625\n",
      "loss: 135 102.118256563\n",
      "loss: 136 96.7485463009\n",
      "loss: 137 91.6656116612\n",
      "loss: 138 86.8542008446\n",
      "loss: 139 82.3011609421\n",
      "loss: 140 77.990223004\n",
      "loss: 141 73.9094480215\n",
      "loss: 142 70.0453166999\n",
      "loss: 143 66.3865735802\n",
      "loss: 144 62.9236470758\n",
      "loss: 145 59.6448326527\n",
      "loss: 146 56.5385680413\n",
      "loss: 147 53.5963589309\n",
      "loss: 148 50.8099413502\n",
      "loss: 149 48.1706323905\n",
      "loss: 150 45.6706563884\n",
      "loss: 151 43.3031031078\n",
      "loss: 152 41.0592933591\n",
      "loss: 153 38.9340851961\n",
      "loss: 154 36.9204497995\n",
      "loss: 155 35.0131188706\n",
      "loss: 156 33.2052682411\n",
      "loss: 157 31.4924174297\n",
      "loss: 158 29.8692721211\n",
      "loss: 159 28.3308291618\n",
      "loss: 160 26.8726948991\n",
      "loss: 161 25.4904516595\n",
      "loss: 162 24.1806512631\n",
      "loss: 163 22.9389302866\n",
      "loss: 164 21.7618704608\n",
      "loss: 165 20.6463109496\n",
      "loss: 166 19.5883632815\n",
      "loss: 167 18.5854624189\n",
      "loss: 168 17.6346400066\n",
      "loss: 169 16.7329494436\n",
      "loss: 170 15.8780251972\n",
      "loss: 171 15.0676020636\n",
      "loss: 172 14.2989170228\n",
      "loss: 173 13.5699542398\n",
      "loss: 174 12.878546012\n",
      "loss: 175 12.2229572891\n",
      "loss: 176 11.6010068728\n",
      "loss: 177 11.0110533942\n",
      "loss: 178 10.4516355788\n",
      "loss: 179 9.92088758588\n",
      "loss: 180 9.41741371317\n",
      "loss: 181 8.93972614781\n",
      "loss: 182 8.48659140053\n",
      "loss: 183 8.05665806411\n",
      "loss: 184 7.64890465537\n",
      "loss: 185 7.26208766925\n",
      "loss: 186 6.89487256301\n",
      "loss: 187 6.54644568469\n",
      "loss: 188 6.2157975445\n",
      "loss: 189 5.90213274753\n",
      "loss: 190 5.60436689712\n",
      "loss: 191 5.32185774782\n",
      "loss: 192 5.05370319286\n",
      "loss: 193 4.79921418403\n",
      "loss: 194 4.55775012343\n",
      "loss: 195 4.32854210469\n",
      "loss: 196 4.11097601194\n",
      "loss: 197 3.90445620791\n",
      "loss: 198 3.70844228791\n",
      "loss: 199 3.52239608305\n",
      "loss: 200 3.34575155298\n",
      "loss: 201 3.17802338539\n",
      "loss: 202 3.01878053153\n",
      "loss: 203 2.86761066806\n",
      "loss: 204 2.72414591704\n",
      "loss: 205 2.58787343151\n",
      "loss: 206 2.45846893294\n",
      "loss: 207 2.3356125088\n",
      "loss: 208 2.21893959348\n",
      "loss: 209 2.10817295751\n",
      "loss: 210 2.00297625786\n",
      "loss: 211 1.90307796752\n",
      "loss: 212 1.80819519389\n",
      "loss: 213 1.71809709416\n",
      "loss: 214 1.63257297538\n",
      "loss: 215 1.5512991371\n",
      "loss: 216 1.47411382431\n",
      "loss: 217 1.40081137233\n",
      "loss: 218 1.33117784021\n",
      "loss: 219 1.26504051325\n",
      "loss: 220 1.20220760852\n",
      "loss: 221 1.14252353164\n",
      "loss: 222 1.08582876082\n",
      "loss: 223 1.03197982568\n",
      "loss: 224 0.980824021052\n",
      "loss: 225 0.932217789981\n",
      "loss: 226 0.886036677876\n",
      "loss: 227 0.842165120384\n",
      "loss: 228 0.800487173276\n",
      "loss: 229 0.760898594031\n",
      "loss: 230 0.723276016004\n",
      "loss: 231 0.687523460661\n",
      "loss: 232 0.653553737614\n",
      "loss: 233 0.621275821792\n",
      "loss: 234 0.590608327612\n",
      "loss: 235 0.561464147429\n",
      "loss: 236 0.53376761826\n",
      "loss: 237 0.507450969983\n",
      "loss: 238 0.482440150372\n",
      "loss: 239 0.458672218645\n",
      "loss: 240 0.43608199878\n",
      "loss: 241 0.414617489282\n",
      "loss: 242 0.39421401672\n",
      "loss: 243 0.374820451677\n",
      "loss: 244 0.356392289322\n",
      "loss: 245 0.338876634223\n",
      "loss: 246 0.322227135068\n",
      "loss: 247 0.306399471382\n",
      "loss: 248 0.291357019089\n",
      "loss: 249 0.277055269572\n",
      "loss: 250 0.263461704737\n",
      "loss: 251 0.250539544303\n",
      "loss: 252 0.23825632779\n",
      "loss: 253 0.226579888335\n",
      "loss: 254 0.215481077583\n",
      "loss: 255 0.204927891408\n",
      "loss: 256 0.194894431743\n",
      "loss: 257 0.185355809617\n",
      "loss: 258 0.176286968198\n",
      "loss: 259 0.167665967142\n",
      "loss: 260 0.15946919682\n",
      "loss: 261 0.151676642563\n",
      "loss: 262 0.144267954435\n",
      "loss: 263 0.137221657014\n",
      "loss: 264 0.13052306708\n",
      "loss: 265 0.124153427525\n",
      "loss: 266 0.118096089549\n",
      "loss: 267 0.112335708728\n",
      "loss: 268 0.106857762181\n",
      "loss: 269 0.101649205224\n",
      "loss: 270 0.0966958205195\n",
      "loss: 271 0.0919858565713\n",
      "loss: 272 0.0875062254685\n",
      "loss: 273 0.0832460962285\n",
      "loss: 274 0.0791957185139\n",
      "loss: 275 0.0753424241258\n",
      "loss: 276 0.0716780387822\n",
      "loss: 277 0.0681930895571\n",
      "loss: 278 0.0648789490528\n",
      "loss: 279 0.0617270648972\n",
      "loss: 280 0.0587285957562\n",
      "loss: 281 0.0558764663725\n",
      "loss: 282 0.0531634915373\n",
      "loss: 283 0.0505838041379\n",
      "loss: 284 0.048130217607\n",
      "loss: 285 0.045795677165\n",
      "loss: 286 0.04357484666\n",
      "loss: 287 0.0414623628528\n",
      "loss: 288 0.0394532086046\n",
      "loss: 289 0.0375420367859\n",
      "loss: 290 0.0357238633128\n",
      "loss: 291 0.0339939162888\n",
      "loss: 292 0.0323484580238\n",
      "loss: 293 0.0307829751685\n",
      "loss: 294 0.0292939414344\n",
      "loss: 295 0.0278772648977\n",
      "loss: 296 0.0265295026338\n",
      "loss: 297 0.0252472593943\n",
      "loss: 298 0.0240271154763\n",
      "loss: 299 0.0228662325503\n",
      "loss: 300 0.0217618199031\n",
      "loss: 301 0.0207111756169\n",
      "loss: 302 0.0197113260135\n",
      "loss: 303 0.018760004651\n",
      "loss: 304 0.0178549659794\n",
      "loss: 305 0.0169937011881\n",
      "loss: 306 0.0161743105122\n",
      "loss: 307 0.0153945870466\n",
      "loss: 308 0.0146526604781\n",
      "loss: 309 0.0139466011945\n",
      "loss: 310 0.0132747221738\n",
      "loss: 311 0.0126353812219\n",
      "loss: 312 0.0120270121023\n",
      "loss: 313 0.0114481811078\n",
      "loss: 314 0.0108973085221\n",
      "loss: 315 0.0103731871306\n",
      "loss: 316 0.00987417720351\n",
      "loss: 317 0.00939933483297\n",
      "loss: 318 0.00894749084723\n",
      "loss: 319 0.00851747663789\n",
      "loss: 320 0.0081081824574\n",
      "loss: 321 0.00771861300858\n",
      "loss: 322 0.00734789232062\n",
      "loss: 323 0.0069950673239\n",
      "loss: 324 0.00665931034891\n",
      "loss: 325 0.00633972359873\n",
      "loss: 326 0.00603549899262\n",
      "loss: 327 0.00574596796137\n",
      "loss: 328 0.00547036578812\n",
      "loss: 329 0.00520804793124\n",
      "loss: 330 0.00495839062692\n",
      "loss: 331 0.00472075175124\n",
      "loss: 332 0.00449456910995\n",
      "loss: 333 0.00427923253939\n",
      "loss: 334 0.00407436175727\n",
      "loss: 335 0.00387925043974\n",
      "loss: 336 0.00369356639817\n",
      "loss: 337 0.0035167664895\n",
      "loss: 338 0.00334847870975\n",
      "loss: 339 0.00318828258003\n",
      "loss: 340 0.0030357732445\n",
      "loss: 341 0.00289059619945\n",
      "loss: 342 0.00275240908437\n",
      "loss: 343 0.00262085023351\n",
      "loss: 344 0.00249561294223\n",
      "loss: 345 0.00237636842501\n",
      "loss: 346 0.00226284251124\n",
      "loss: 347 0.00215478018806\n",
      "loss: 348 0.00205190640147\n",
      "loss: 349 0.00195394144037\n",
      "loss: 350 0.00186067851132\n",
      "loss: 351 0.00177188058905\n",
      "loss: 352 0.00168734426134\n",
      "loss: 353 0.00160688557807\n",
      "loss: 354 0.00153026841308\n",
      "loss: 355 0.00145730331864\n",
      "loss: 356 0.00138783457017\n",
      "loss: 357 0.00132169390753\n",
      "loss: 358 0.0012587138102\n",
      "loss: 359 0.00119875307552\n",
      "loss: 360 0.00114165432274\n",
      "loss: 361 0.00108728427653\n",
      "loss: 362 0.00103551283911\n",
      "loss: 363 0.000986216809046\n",
      "loss: 364 0.000939290149831\n",
      "loss: 365 0.000894592799662\n",
      "loss: 366 0.000852031964019\n",
      "loss: 367 0.00081149880241\n",
      "loss: 368 0.000772908564355\n",
      "loss: 369 0.000736157206822\n",
      "loss: 370 0.000701162348793\n",
      "loss: 371 0.000667838253262\n",
      "loss: 372 0.000636103307882\n",
      "loss: 373 0.000605883379158\n",
      "loss: 374 0.000577109201173\n",
      "loss: 375 0.000549699940375\n",
      "loss: 376 0.000523598672227\n",
      "loss: 377 0.000498740487557\n",
      "loss: 378 0.000475064380206\n",
      "loss: 379 0.000452517912977\n",
      "loss: 380 0.000431044763776\n",
      "loss: 381 0.000410595787318\n",
      "loss: 382 0.000391120756928\n",
      "loss: 383 0.000372572399547\n",
      "loss: 384 0.000354909493771\n",
      "loss: 385 0.000338084210787\n",
      "loss: 386 0.000322058604265\n",
      "loss: 387 0.000306796951052\n",
      "loss: 388 0.000292259431794\n",
      "loss: 389 0.000278414489256\n",
      "loss: 390 0.000265226115852\n",
      "loss: 391 0.000252664878912\n",
      "loss: 392 0.000240700883975\n",
      "loss: 393 0.00022930767067\n",
      "loss: 394 0.000218455229421\n",
      "loss: 395 0.000208118354448\n",
      "loss: 396 0.000198271654236\n",
      "loss: 397 0.00018889075499\n",
      "loss: 398 0.000179957119274\n",
      "loss: 399 0.000171445861204\n",
      "loss: 400 0.000163339465731\n",
      "loss: 401 0.000155617185894\n",
      "loss: 402 0.000148260822045\n",
      "loss: 403 0.000141254732336\n",
      "loss: 404 0.000134580636953\n",
      "loss: 405 0.000128222490462\n",
      "loss: 406 0.00012216500082\n",
      "loss: 407 0.000116395092222\n",
      "loss: 408 0.000110898084097\n",
      "loss: 409 0.000105662584667\n",
      "loss: 410 0.000100674847917\n",
      "loss: 411 9.59230018404e-05\n",
      "loss: 412 9.13960149896e-05\n",
      "loss: 413 8.70843265375e-05\n",
      "loss: 414 8.29756129494e-05\n",
      "loss: 415 7.90618548393e-05\n",
      "loss: 416 7.5333620873e-05\n",
      "loss: 417 7.17815324938e-05\n",
      "loss: 418 6.83971693542e-05\n",
      "loss: 419 6.5172632951e-05\n",
      "loss: 420 6.21010322098e-05\n",
      "loss: 421 5.91747235826e-05\n",
      "loss: 422 5.63865573167e-05\n",
      "loss: 423 5.37303339309e-05\n",
      "loss: 424 5.11991836557e-05\n",
      "loss: 425 4.87878646945e-05\n",
      "loss: 426 4.64906235146e-05\n",
      "loss: 427 4.4301723493e-05\n",
      "loss: 428 4.22161171688e-05\n",
      "loss: 429 4.02289371939e-05\n",
      "loss: 430 3.83356364196e-05\n",
      "loss: 431 3.6531745967e-05\n",
      "loss: 432 3.4813530639e-05\n",
      "loss: 433 3.31758874567e-05\n",
      "loss: 434 3.16155075388e-05\n",
      "loss: 435 3.01285109609e-05\n",
      "loss: 436 2.87118271128e-05\n",
      "loss: 437 2.73620715379e-05\n",
      "loss: 438 2.60760730511e-05\n",
      "loss: 439 2.48506569726e-05\n",
      "loss: 440 2.36827575593e-05\n",
      "loss: 441 2.25700394392e-05\n",
      "loss: 442 2.15099309152e-05\n",
      "loss: 443 2.04995815467e-05\n",
      "loss: 444 1.95367538993e-05\n",
      "loss: 445 1.86193210875e-05\n",
      "loss: 446 1.77449887238e-05\n",
      "loss: 447 1.69118895377e-05\n",
      "loss: 448 1.61180830392e-05\n",
      "loss: 449 1.53616090337e-05\n",
      "loss: 450 1.46407101715e-05\n",
      "loss: 451 1.39537365537e-05\n",
      "loss: 452 1.329917328e-05\n",
      "loss: 453 1.26753774483e-05\n",
      "loss: 454 1.20808878779e-05\n",
      "loss: 455 1.15143625245e-05\n",
      "loss: 456 1.09744432458e-05\n",
      "loss: 457 1.0459923602e-05\n",
      "loss: 458 9.9695557818e-06\n",
      "loss: 459 9.50234145363e-06\n",
      "loss: 460 9.056996933e-06\n",
      "loss: 461 8.63277695917e-06\n",
      "loss: 462 8.2282850746e-06\n",
      "loss: 463 7.8428484843e-06\n",
      "loss: 464 7.47551395705e-06\n",
      "loss: 465 7.12541983666e-06\n",
      "loss: 466 6.7917294982e-06\n",
      "loss: 467 6.47375053897e-06\n",
      "loss: 468 6.17067242888e-06\n",
      "loss: 469 5.88184615411e-06\n",
      "loss: 470 5.60660202631e-06\n",
      "loss: 471 5.34422126269e-06\n",
      "loss: 472 5.09415690927e-06\n",
      "loss: 473 4.85581905276e-06\n",
      "loss: 474 4.62866869006e-06\n",
      "loss: 475 4.41218943216e-06\n",
      "loss: 476 4.20583456882e-06\n",
      "loss: 477 4.00915007642e-06\n",
      "loss: 478 3.8216989666e-06\n",
      "loss: 479 3.64304075876e-06\n",
      "loss: 480 3.47278086856e-06\n",
      "loss: 481 3.3104783635e-06\n",
      "loss: 482 3.15576094658e-06\n",
      "loss: 483 3.00829753789e-06\n",
      "loss: 484 2.86776835464e-06\n",
      "loss: 485 2.73381024535e-06\n",
      "loss: 486 2.60611945281e-06\n",
      "loss: 487 2.48439671814e-06\n",
      "loss: 488 2.36837180103e-06\n",
      "loss: 489 2.25779566884e-06\n",
      "loss: 490 2.15239284047e-06\n",
      "loss: 491 2.05192308212e-06\n",
      "loss: 492 1.95614389211e-06\n",
      "loss: 493 1.86484306738e-06\n",
      "loss: 494 1.77781987301e-06\n",
      "loss: 495 1.69485635141e-06\n",
      "loss: 496 1.61579792511e-06\n",
      "loss: 497 1.54042037089e-06\n",
      "loss: 498 1.46858245635e-06\n",
      "loss: 499 1.40009075025e-06\n"
     ]
    }
   ],
   "source": [
    "# perform forward and backwards passes\n",
    "for t in range(num_passes):\n",
    "    # the forward pass - propagate the inputs through the network\n",
    "    # layer by layer until it reaches the output layer. To propagate\n",
    "    # it through the first layer, multiply the inputs by the first\n",
    "    # layer weights.\n",
    "    \n",
    "    # dot product -- cosine similarity - 64 x 100\n",
    "    h = x.dot(w1)\n",
    "    # don't allow negative numbers in array\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    # prediction = x * weight * weight2\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print('loss:', t, loss)\n",
    "    \n",
    "    # backpropagation - computes gradients of w1 and w2 w/r/t loss\n",
    "    # compute error at the output and distribute it back to hidden layer\n",
    "    # backprop requires known output for each input value so that you can \n",
    "    # compute the error (supervised). You can use whatever error function\n",
    "    # you like, or you can even use a weighted sum of different error\n",
    "    # functions for individual training examples.\n",
    "    \n",
    "    # gradient descent gives us the weights minimizing the error\n",
    "    \n",
    "    # 2 * error or squared error?\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # error * weighted/capped X\n",
    "    # this gives us new adjustment for second sets of weights\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    # error gradient * weights 2\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    # no negatives again\n",
    "    grad_h[h < 0] = 0\n",
    "    # data * error gradient * weights 2\n",
    "    # this gives us new adjustment for first set of weights\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss converges on 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This gets super complicated when we have more than one hidden layer.\n",
    "# Torch provides the implementation for forward/backward passes using\n",
    "# automatic differentiation -- basically the generalization of backprop.\n",
    "# torch.autograd does this. It defines a computational graph where\n",
    "# nodes in the graph are tensors and edges are functions that produce\n",
    "# output tensors from input tensors. This greatly facilitates backprop --\n",
    "# allows you to easily compute gradients. \n",
    "\n",
    "# Variable represents a node in a computational graph.\n",
    "# x = Variable()\n",
    "# x.data // a tensor\n",
    "# x.grad // holds the gradient of x\n",
    "\n",
    "# Variables have same API as Tensors -- nearly any operation on a variable\n",
    "# is one that can be done on a tensor; the difference is that using a\n",
    "# variable defines a computational graph.\n",
    "\n",
    "# Using torch to implement a 2-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "# create the random input/output data -- these won't require gradients.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# we want to compute gradients w/r/t these during backprop\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29792916.0\n",
      "1 28371110.0\n",
      "2 32703822.0\n",
      "3 37737352.0\n",
      "4 37553976.0\n",
      "5 29106612.0\n",
      "6 17101122.0\n",
      "7 8252016.0\n",
      "8 3861294.25\n",
      "9 2052118.375\n",
      "10 1312504.0\n",
      "11 970250.875\n",
      "12 778663.5\n",
      "13 650675.3125\n",
      "14 554636.3125\n",
      "15 477834.3125\n",
      "16 414490.34375\n",
      "17 361360.90625\n",
      "18 316375.6875\n",
      "19 277990.4375\n",
      "20 245141.25\n",
      "21 216942.765625\n",
      "22 192529.8125\n",
      "23 171336.984375\n",
      "24 152865.1875\n",
      "25 136702.96875\n",
      "26 122503.7890625\n",
      "27 110017.7265625\n",
      "28 98992.0859375\n",
      "29 89240.1640625\n",
      "30 80598.0859375\n",
      "31 72910.625\n",
      "32 66063.9453125\n",
      "33 59950.3046875\n",
      "34 54486.09765625\n",
      "35 49594.7265625\n",
      "36 45216.48828125\n",
      "37 41278.44140625\n",
      "38 37730.6875\n",
      "39 34531.2109375\n",
      "40 31640.08203125\n",
      "41 29023.005859375\n",
      "42 26652.130859375\n",
      "43 24498.955078125\n",
      "44 22543.193359375\n",
      "45 20763.0625\n",
      "46 19141.328125\n",
      "47 17662.58984375\n",
      "48 16313.154296875\n",
      "49 15079.87109375\n",
      "50 13951.4541015625\n",
      "51 12917.8193359375\n",
      "52 11970.1591796875\n",
      "53 11100.6982421875\n",
      "54 10301.1669921875\n",
      "55 9566.5146484375\n",
      "56 8890.6904296875\n",
      "57 8269.1064453125\n",
      "58 7696.71435546875\n",
      "59 7168.49560546875\n",
      "60 6680.97314453125\n",
      "61 6230.6318359375\n",
      "62 5814.216796875\n",
      "63 5429.81640625\n",
      "64 5074.02587890625\n",
      "65 4744.3916015625\n",
      "66 4438.8623046875\n",
      "67 4155.296875\n",
      "68 3892.023681640625\n",
      "69 3647.359619140625\n",
      "70 3419.90380859375\n",
      "71 3208.32666015625\n",
      "72 3011.34521484375\n",
      "73 2827.943603515625\n",
      "74 2656.97265625\n",
      "75 2497.528076171875\n",
      "76 2348.839599609375\n",
      "77 2209.99951171875\n",
      "78 2080.3134765625\n",
      "79 1959.094970703125\n",
      "80 1845.748046875\n",
      "81 1739.7117919921875\n",
      "82 1640.4161376953125\n",
      "83 1547.421630859375\n",
      "84 1460.2919921875\n",
      "85 1378.6500244140625\n",
      "86 1302.0804443359375\n",
      "87 1230.24365234375\n",
      "88 1162.78076171875\n",
      "89 1099.444580078125\n",
      "90 1039.9222412109375\n",
      "91 983.97265625\n",
      "92 931.3607177734375\n",
      "93 881.8595581054688\n",
      "94 835.2588500976562\n",
      "95 791.3836669921875\n",
      "96 750.0545654296875\n",
      "97 711.10400390625\n",
      "98 674.3764038085938\n",
      "99 639.7415771484375\n",
      "100 607.0662841796875\n",
      "101 576.2384643554688\n",
      "102 547.1372680664062\n",
      "103 519.6436767578125\n",
      "104 493.6732177734375\n",
      "105 469.1392517089844\n",
      "106 445.93951416015625\n",
      "107 424.003662109375\n",
      "108 403.25299072265625\n",
      "109 383.64166259765625\n",
      "110 365.08538818359375\n",
      "111 347.5157165527344\n",
      "112 330.8700866699219\n",
      "113 315.1033935546875\n",
      "114 300.1632080078125\n",
      "115 285.9917907714844\n",
      "116 272.55596923828125\n",
      "117 259.81097412109375\n",
      "118 247.71746826171875\n",
      "119 236.23802185058594\n",
      "120 225.3410186767578\n",
      "121 214.99041748046875\n",
      "122 205.16232299804688\n",
      "123 195.82452392578125\n",
      "124 186.94943237304688\n",
      "125 178.51295471191406\n",
      "126 170.49501037597656\n",
      "127 162.86636352539062\n",
      "128 155.61097717285156\n",
      "129 148.70608520507812\n",
      "130 142.1368865966797\n",
      "131 135.88247680664062\n",
      "132 129.9290313720703\n",
      "133 124.25698852539062\n",
      "134 118.85382080078125\n",
      "135 113.70551300048828\n",
      "136 108.80024719238281\n",
      "137 104.12519836425781\n",
      "138 99.66730499267578\n",
      "139 95.41724395751953\n",
      "140 91.36408233642578\n",
      "141 87.49925231933594\n",
      "142 83.81182098388672\n",
      "143 80.29234313964844\n",
      "144 76.93421173095703\n",
      "145 73.7269058227539\n",
      "146 70.6647720336914\n",
      "147 67.74002838134766\n",
      "148 64.94620513916016\n",
      "149 62.27666473388672\n",
      "150 59.7263298034668\n",
      "151 57.288902282714844\n",
      "152 54.959224700927734\n",
      "153 52.729454040527344\n",
      "154 50.60000228881836\n",
      "155 48.561920166015625\n",
      "156 46.6131477355957\n",
      "157 44.74794006347656\n",
      "158 42.963008880615234\n",
      "159 41.25479507446289\n",
      "160 39.62116622924805\n",
      "161 38.05574417114258\n",
      "162 36.55651092529297\n",
      "163 35.121456146240234\n",
      "164 33.746829986572266\n",
      "165 32.43016815185547\n",
      "166 31.168901443481445\n",
      "167 29.960134506225586\n",
      "168 28.801708221435547\n",
      "169 27.691326141357422\n",
      "170 26.62639808654785\n",
      "171 25.60597038269043\n",
      "172 24.627073287963867\n",
      "173 23.688426971435547\n",
      "174 22.787927627563477\n",
      "175 21.924671173095703\n",
      "176 21.09610939025879\n",
      "177 20.300783157348633\n",
      "178 19.53754997253418\n",
      "179 18.805700302124023\n",
      "180 18.101892471313477\n",
      "181 17.42585563659668\n",
      "182 16.77760124206543\n",
      "183 16.15500831604004\n",
      "184 15.556801795959473\n",
      "185 14.982773780822754\n",
      "186 14.430910110473633\n",
      "187 13.900577545166016\n",
      "188 13.391590118408203\n",
      "189 12.902000427246094\n",
      "190 12.431648254394531\n",
      "191 11.979789733886719\n",
      "192 11.545132637023926\n",
      "193 11.127517700195312\n",
      "194 10.72591781616211\n",
      "195 10.339719772338867\n",
      "196 9.968424797058105\n",
      "197 9.611342430114746\n",
      "198 9.267778396606445\n",
      "199 8.937028884887695\n",
      "200 8.618934631347656\n",
      "201 8.313125610351562\n",
      "202 8.018951416015625\n",
      "203 7.735513210296631\n",
      "204 7.462937355041504\n",
      "205 7.200274467468262\n",
      "206 6.947570323944092\n",
      "207 6.704257488250732\n",
      "208 6.470275402069092\n",
      "209 6.244507312774658\n",
      "210 6.027486324310303\n",
      "211 5.818212985992432\n",
      "212 5.616625785827637\n",
      "213 5.42272424697876\n",
      "214 5.235596656799316\n",
      "215 5.055514812469482\n",
      "216 4.881786346435547\n",
      "217 4.714634895324707\n",
      "218 4.553244590759277\n",
      "219 4.397872447967529\n",
      "220 4.248107433319092\n",
      "221 4.103671073913574\n",
      "222 3.9644792079925537\n",
      "223 3.8301353454589844\n",
      "224 3.700683832168579\n",
      "225 3.575866937637329\n",
      "226 3.4555652141571045\n",
      "227 3.339495897293091\n",
      "228 3.2274062633514404\n",
      "229 3.119429111480713\n",
      "230 3.0151288509368896\n",
      "231 2.9147531986236572\n",
      "232 2.8176627159118652\n",
      "233 2.7240867614746094\n",
      "234 2.6338114738464355\n",
      "235 2.546570301055908\n",
      "236 2.4624671936035156\n",
      "237 2.381209373474121\n",
      "238 2.3029520511627197\n",
      "239 2.227156162261963\n",
      "240 2.154045343399048\n",
      "241 2.0836143493652344\n",
      "242 2.015434503555298\n",
      "243 1.9496183395385742\n",
      "244 1.8861629962921143\n",
      "245 1.8247896432876587\n",
      "246 1.7654552459716797\n",
      "247 1.7081931829452515\n",
      "248 1.652875304222107\n",
      "249 1.5994694232940674\n",
      "250 1.547940731048584\n",
      "251 1.4980015754699707\n",
      "252 1.4497966766357422\n",
      "253 1.4033409357070923\n",
      "254 1.358373761177063\n",
      "255 1.3148493766784668\n",
      "256 1.2727919816970825\n",
      "257 1.2322020530700684\n",
      "258 1.1929056644439697\n",
      "259 1.154929518699646\n",
      "260 1.1181938648223877\n",
      "261 1.0827357769012451\n",
      "262 1.0484851598739624\n",
      "263 1.0152722597122192\n",
      "264 0.9831801652908325\n",
      "265 0.9521418213844299\n",
      "266 0.9221733212471008\n",
      "267 0.893226146697998\n",
      "268 0.8651500940322876\n",
      "269 0.8380144834518433\n",
      "270 0.8116937279701233\n",
      "271 0.7863205671310425\n",
      "272 0.7617594599723816\n",
      "273 0.737971305847168\n",
      "274 0.7150364518165588\n",
      "275 0.6927701234817505\n",
      "276 0.671262800693512\n",
      "277 0.6504198312759399\n",
      "278 0.6301910877227783\n",
      "279 0.61067795753479\n",
      "280 0.5917913913726807\n",
      "281 0.5735610723495483\n",
      "282 0.5558591485023499\n",
      "283 0.5387879610061646\n",
      "284 0.5221247673034668\n",
      "285 0.5061277151107788\n",
      "286 0.49056383967399597\n",
      "287 0.4755094349384308\n",
      "288 0.4609464108943939\n",
      "289 0.4468601942062378\n",
      "290 0.4332062602043152\n",
      "291 0.4199889600276947\n",
      "292 0.4071594476699829\n",
      "293 0.3948049545288086\n",
      "294 0.3827539086341858\n",
      "295 0.37110215425491333\n",
      "296 0.35982635617256165\n",
      "297 0.34896403551101685\n",
      "298 0.338373064994812\n",
      "299 0.3280941843986511\n",
      "300 0.3182107210159302\n",
      "301 0.30855849385261536\n",
      "302 0.29925447702407837\n",
      "303 0.2902752757072449\n",
      "304 0.28154316544532776\n",
      "305 0.2730269730091095\n",
      "306 0.264828622341156\n",
      "307 0.25687578320503235\n",
      "308 0.24919608235359192\n",
      "309 0.2416764795780182\n",
      "310 0.23445218801498413\n",
      "311 0.22744573652744293\n",
      "312 0.2206544429063797\n",
      "313 0.21411752700805664\n",
      "314 0.20769113302230835\n",
      "315 0.20149396359920502\n",
      "316 0.1954953968524933\n",
      "317 0.18970344960689545\n",
      "318 0.18404461443424225\n",
      "319 0.17858849465847015\n",
      "320 0.17330029606819153\n",
      "321 0.16814415156841278\n",
      "322 0.16316844522953033\n",
      "323 0.15835681557655334\n",
      "324 0.15365873277187347\n",
      "325 0.1491268128156662\n",
      "326 0.144708514213562\n",
      "327 0.14044618606567383\n",
      "328 0.13628092408180237\n",
      "329 0.132307767868042\n",
      "330 0.1283949762582779\n",
      "331 0.12462618201971054\n",
      "332 0.1209685429930687\n",
      "333 0.1174231469631195\n",
      "334 0.11399349570274353\n",
      "335 0.11064071208238602\n",
      "336 0.1073966771364212\n",
      "337 0.10426623374223709\n",
      "338 0.10123229026794434\n",
      "339 0.09827034175395966\n",
      "340 0.09538870304822922\n",
      "341 0.09261899441480637\n",
      "342 0.08989815413951874\n",
      "343 0.08730136603116989\n",
      "344 0.08475154638290405\n",
      "345 0.08230625838041306\n",
      "346 0.07991260290145874\n",
      "347 0.07758660614490509\n",
      "348 0.07535269111394882\n",
      "349 0.07315849512815475\n",
      "350 0.07105547189712524\n",
      "351 0.06897637248039246\n",
      "352 0.06698476523160934\n",
      "353 0.06505491584539413\n",
      "354 0.06319060921669006\n",
      "355 0.061363209038972855\n",
      "356 0.05959589406847954\n",
      "357 0.05788327008485794\n",
      "358 0.05621472746133804\n",
      "359 0.05459544062614441\n",
      "360 0.05303161218762398\n",
      "361 0.05150134488940239\n",
      "362 0.050022415816783905\n",
      "363 0.04859219491481781\n",
      "364 0.04720364883542061\n",
      "365 0.04584743082523346\n",
      "366 0.04453849047422409\n",
      "367 0.04326188564300537\n",
      "368 0.04202655330300331\n",
      "369 0.04082676023244858\n",
      "370 0.03967573493719101\n",
      "371 0.03854885324835777\n",
      "372 0.0374479703605175\n",
      "373 0.03638596460223198\n",
      "374 0.035347651690244675\n",
      "375 0.034344691783189774\n",
      "376 0.03336130082607269\n",
      "377 0.03242531046271324\n",
      "378 0.03150881081819534\n",
      "379 0.030606849119067192\n",
      "380 0.029747530817985535\n",
      "381 0.028905199840664864\n",
      "382 0.028085799887776375\n",
      "383 0.027305372059345245\n",
      "384 0.026531947776675224\n",
      "385 0.025799717754125595\n",
      "386 0.025061553344130516\n",
      "387 0.024364879354834557\n",
      "388 0.02367495745420456\n",
      "389 0.023015065118670464\n",
      "390 0.022375520318746567\n",
      "391 0.02175857499241829\n",
      "392 0.02114155888557434\n",
      "393 0.020552044734358788\n",
      "394 0.019982794299721718\n",
      "395 0.019420571625232697\n",
      "396 0.018893830478191376\n",
      "397 0.018363790586590767\n",
      "398 0.01785815693438053\n",
      "399 0.017361924052238464\n",
      "400 0.016879357397556305\n",
      "401 0.016411783173680305\n",
      "402 0.01595667563378811\n",
      "403 0.015523107722401619\n",
      "404 0.01509540993720293\n",
      "405 0.014680511318147182\n",
      "406 0.014280772767961025\n",
      "407 0.013882583007216454\n",
      "408 0.013501331210136414\n",
      "409 0.013135865330696106\n",
      "410 0.012776857241988182\n",
      "411 0.01243264228105545\n",
      "412 0.012091036885976791\n",
      "413 0.011762860231101513\n",
      "414 0.011446286924183369\n",
      "415 0.011144079267978668\n",
      "416 0.010846245102584362\n",
      "417 0.010543107986450195\n",
      "418 0.010262181051075459\n",
      "419 0.009993813931941986\n",
      "420 0.009724011644721031\n",
      "421 0.009462212212383747\n",
      "422 0.009208275005221367\n",
      "423 0.008963070809841156\n",
      "424 0.008720479905605316\n",
      "425 0.008489889092743397\n",
      "426 0.008256006054580212\n",
      "427 0.008039423264563084\n",
      "428 0.00782830361276865\n",
      "429 0.007624321151524782\n",
      "430 0.007423368748277426\n",
      "431 0.007225425913929939\n",
      "432 0.0070375483483076096\n",
      "433 0.00685106310993433\n",
      "434 0.006673599127680063\n",
      "435 0.006498031783849001\n",
      "436 0.006327138282358646\n",
      "437 0.006162566598504782\n",
      "438 0.005997858010232449\n",
      "439 0.005846539046615362\n",
      "440 0.005695228464901447\n",
      "441 0.005552050657570362\n",
      "442 0.005402800161391497\n",
      "443 0.005267004948109388\n",
      "444 0.005126124247908592\n",
      "445 0.004999284166842699\n",
      "446 0.004872388206422329\n",
      "447 0.004750930238515139\n",
      "448 0.004627683199942112\n",
      "449 0.004511551931500435\n",
      "450 0.004395071882754564\n",
      "451 0.0042869639582931995\n",
      "452 0.004178123082965612\n",
      "453 0.0040759677067399025\n",
      "454 0.003973666112869978\n",
      "455 0.0038740690797567368\n",
      "456 0.003774740267544985\n",
      "457 0.0036819162778556347\n",
      "458 0.003592577064409852\n",
      "459 0.003502602456137538\n",
      "460 0.0034171519801020622\n",
      "461 0.003331840271130204\n",
      "462 0.003251799615100026\n",
      "463 0.0031731771305203438\n",
      "464 0.003096103435382247\n",
      "465 0.003020488889887929\n",
      "466 0.002947079250589013\n",
      "467 0.002872467041015625\n",
      "468 0.002805683994665742\n",
      "469 0.0027389870956540108\n",
      "470 0.0026718322187662125\n",
      "471 0.00260632811114192\n",
      "472 0.0025470328982919455\n",
      "473 0.0024868997279554605\n",
      "474 0.0024277367629110813\n",
      "475 0.0023703770712018013\n",
      "476 0.002316464204341173\n",
      "477 0.002259519649669528\n",
      "478 0.002207554643973708\n",
      "479 0.002157768467441201\n",
      "480 0.002108148066326976\n",
      "481 0.0020594995003193617\n",
      "482 0.0020110802724957466\n",
      "483 0.0019640137907117605\n",
      "484 0.0019186040153726935\n",
      "485 0.0018754751654341817\n",
      "486 0.0018330476013943553\n",
      "487 0.0017923183040693402\n",
      "488 0.0017516565276309848\n",
      "489 0.001710462267510593\n",
      "490 0.0016740491846576333\n",
      "491 0.0016362996539101005\n",
      "492 0.001600143383257091\n",
      "493 0.0015639712801203132\n",
      "494 0.0015296833589673042\n",
      "495 0.001497121644206345\n",
      "496 0.001463292632251978\n",
      "497 0.0014289651298895478\n",
      "498 0.001398902852088213\n",
      "499 0.0013700489653274417\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: basically the same as numpy operations, but \n",
    "    # don't need to remember intermediate variables.\n",
    "    # mm is matrix multiplication.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute loss -- loss.shape == (1,) (a Variable)\n",
    "    # loss.data.shape == (1,) (a Tensor)\n",
    "    # loss.data[0] is a scalar value holding the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    # Backward pass can be computed from any Variable throughout\n",
    "    # the network associated with that Variable. Computes loss\n",
    "    # gradient w/r/t all Variables with requires_grad=True. So\n",
    "    # w1.grad and w2.grad will be Variables holding the gradient\n",
    "    # of the loss w/r/t w1 and w2. It knows to do this since\n",
    "    # loss is the output of y_pred and y, and y_pred is the product\n",
    "    # of x, w1, w2. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights with gradient descent. w1.data and w2.data are\n",
    "    # Tensors, w1.grad and w2.grad are Variables and w1.grad.data\n",
    "    # and w2.grad.data are tensors.\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    # Zero the gradients\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again, converges towards 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
