{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief neural network breakdown\n",
    "**Regular NNs** are made up of layers of units (\"neurons\") with learnable weights and biases. Each unit is fully connected to all neurons in the previous layer. Each unit receives inputs, performs a dot product operation, and then optionally follows it with non-linearity. The whole network still expresses a single differentiable score function (hence the ability to predict). There is an input layer and output layer. The layers in between are called **hidden layers**. Last fully connected layer produces the output scores across all, say, classes (so a 10-class predictor would have an output layer of size m x 10).  \n",
    "\n",
    "Neural networks are comprised of: \n",
    "* **Layers**: main building block. Operate sort of like a data filter. Extract representations from the data fed in. A network is chaining together simple layers to form more complex representations -- data distillation.\n",
    "* **Loss function**: how the network measures how well it's doing; what it tries to optimize\n",
    "* **Optimizer**: how it updates itself (stochastic gradient descent, adam, etc) based on loss fn. I.e. how it updates its weights. \n",
    "\n",
    "## Core Keras layers\n",
    "* **Dense**: fully-connected layer (where kernel is the weights - so just doing all pairwise element multiplication between data and weights, hence the dense-ness)\n",
    "  * `output = activation_fn(dot(input, kernel) + bias)`\n",
    "* **Activation**: activation layer; just applies the activation function to the output matrix\n",
    "* **Dropout**: applies dropout to the input, meaning it sets a fraction of the input units to 0 during training (helps prevent overfitting and speeds up computation - the more layers and interconnections there are, the more complex a representation can be learned, which can lead to overfitting. \"Thinning\" out the networks helps prevent this.)\n",
    "* **Flatten**: flattens the input (1D)\n",
    "* **Repeat**: repeat the input `n` times\n",
    "* **Softmax**: \n",
    "* **Convolutional layers**: best for images. Regular networks as described above (a sequence of dense layers) don't scale well and would lead to overfitting. CNNs typically arrange their layers in a 3D shape: width, height, depth. Basically a 3D network where units are only connected to local regions in the previous layer. \n",
    "* Pooling layers:\n",
    "* Locally-connected layers:\n",
    "* Recurrent layers: \n",
    "* Embedding layers: \n",
    "* See more [here](https://keras.io/layers/core/)\n",
    "\n",
    "### CNNs\n",
    "Typical architecture of a CNN classifier is:\n",
    "![title](assets/cnn_architecture.jpeg)\n",
    "1. `INPUT` layer (ex: 64x64x3 - raw pixel values)\n",
    "2. `CONV` layer (has units connected to local regions of input layer and computes dot product between region and units. for ex if there are 20 regions would end up with 64x64x20) \n",
    "3. `RELU` layer (element-wise activation function, like thresholding at 0 -- all cells >= 0. (64x64x20)\n",
    "4. `POOL` layer (downsamples along input dimensions: 64x64x20 --> 32x32x20)\n",
    "5. `DENSE` layer (fully connected output layer computes class scores: 1x1x10 if we have 10 classes)\n",
    "\n",
    "Each layer transforms a 3D input to 3D output (volume). Transforms one volume of activations to another through differentiable functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
