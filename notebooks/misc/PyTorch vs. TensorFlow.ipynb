{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TensorFlow vs. PyTorch\n",
    "# TensorFlow is a Google Brain project\n",
    "# PyTorch is based off lua Torch framework developed at FB (not just wrapper; rewritten to run fast & natively)\n",
    "\n",
    "# To compare them, code an approximator that will find\n",
    "# unknown parameter phi given data X: f(x) = x^phi\n",
    "# using stochastic gradient descent (obviously can find\n",
    "# an analytic solution for this, but just an example).\n",
    "\n",
    "# pytorch's optimize module contains optimization\n",
    "\n",
    "## Static vs. Dynamic graphs\n",
    "\n",
    "# TensorFlow uses static computational graphs -- defined once and\n",
    "# is executed over and over again -- optimized up front (makes it easy\n",
    "# to distribute/amortize it among different machines)\n",
    "# PyTorch uses dynamic computational graphs -- each forward pass defines\n",
    "# a new graph. Provides more granular control over the control flow.\n",
    "# For example, might want to perform a different computation for each\n",
    "# data point. In TF, need to use TF.scan to embed this in the graph's\n",
    "# loop. In Torch, can control the computation that differs for each \n",
    "# input in your own normal imperative loop. In TF everything works\n",
    "# via the session. Different architectures might want to use static or\n",
    "# dynamic graphs -- for example, RNNs require dynamic inputs (like changing\n",
    "# sentence length). Since define TF graphs once, at the beginning, will\n",
    "# need to pick a max input length and pad the rest of the input sentences\n",
    "# by 0. \n",
    "\n",
    "## TF also has better visualization & debugging.\n",
    "\n",
    "## Deployment\n",
    "\n",
    "# TF is much better for deployment -- has TensorFLow Serving which\n",
    "# is a framework for deploying models on a specialized server (also\n",
    "# supports mobile!) Though can still use PyTorch with flask or whatever.\n",
    "\n",
    "# TF also supports distributed training.\n",
    "\n",
    "## Framework\n",
    "\n",
    "# TF feels more like a low-level library than a framework. But things like\n",
    "# Keras can run on top of TF.\n",
    "\n",
    "## Adoption\n",
    "\n",
    "# TF: well-documented, go-to, tons of tutorials\n",
    "# Torch: still in beta as of summer 2017\n",
    "\n",
    "# Adam optimization algorithm: the best current performance\n",
    "# optimizer out there. Is invarient to scaling the gradient,\n",
    "# which means that when you do k * f(x) on an iteration,\n",
    "# there is no effect on the performance. Will converge \n",
    "# even if f(x) changes with time. Don't need to decrease\n",
    "# step size after a certain number of epochs -- naturally does\n",
    "# step size annealing.\n",
    "# MORE HERE: http://ruder.io/optimizing-gradient-descent/index.html\n",
    "# and here: https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y, y_hat):\n",
    "    return torch.sqrt(torch.mean((y - y_hat).pow(2).sum()))\n",
    "\n",
    "# performs a forward pass on the function\n",
    "# takes in two pytorch Variables. Variable is a wrapper\n",
    "# around a tensor that is a node around a computational graph.\n",
    "def forward(x, e):\n",
    "    return x.pow(e.repeat(x.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of datapoints\n",
    "n = 1000\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 5e6\n",
    "\n",
    "# create random datapoints & define model\n",
    "x = Variable(torch.rand(n) * 10, requires_grad=False)\n",
    "\n",
    "# model parameters - the exponent and the exponent squared\n",
    "exp = Variable(torch.FloatTensor([2.0]), requires_grad=False)\n",
    "exp_hat = Variable(torch.FloatTensor([4.0]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 7.8744\n",
       " 6.9353\n",
       " 6.6861\n",
       " 6.8646\n",
       " 3.5993\n",
       " 1.5355\n",
       " 5.8961\n",
       " 5.6437\n",
       " 3.1669\n",
       " 0.0022\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns torch.Size([10])\n",
    "x.size()\n",
    "# returns 10\n",
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeats exp 10 times\n",
    "exp.repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exploit our forward pass function to merely compute x^exp\n",
    "# and get our output data\n",
    "y = forward(x, exp)\n",
    "# in this case x^2\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t 107778.8359375 -1195734859776.0\n",
      "Iteration 1 \t inf nan\n",
      "Iteration 2 \t nan nan\n",
      "Iteration 3 \t nan nan\n",
      "Iteration 4 \t nan nan\n",
      "Iteration 5 \t nan nan\n",
      "Iteration 6 \t nan nan\n",
      "Iteration 7 \t nan nan\n",
      "Iteration 8 \t nan nan\n",
      "Iteration 9 \t nan nan\n",
      "Iteration 10 \t nan nan\n",
      "Iteration 11 \t nan nan\n",
      "Iteration 12 \t nan nan\n",
      "Iteration 13 \t nan nan\n",
      "Iteration 14 \t nan nan\n",
      "Iteration 15 \t nan nan\n",
      "Iteration 16 \t nan nan\n",
      "Iteration 17 \t nan nan\n",
      "Iteration 18 \t nan nan\n",
      "Iteration 19 \t nan nan\n",
      "Iteration 20 \t nan nan\n",
      "Iteration 21 \t nan nan\n",
      "Iteration 22 \t nan nan\n",
      "Iteration 23 \t nan nan\n",
      "Iteration 24 \t nan nan\n",
      "Iteration 25 \t nan nan\n",
      "Iteration 26 \t nan nan\n",
      "Iteration 27 \t nan nan\n",
      "Iteration 28 \t nan nan\n",
      "Iteration 29 \t nan nan\n",
      "Iteration 30 \t nan nan\n",
      "Iteration 31 \t nan nan\n",
      "Iteration 32 \t nan nan\n",
      "Iteration 33 \t nan nan\n",
      "Iteration 34 \t nan nan\n",
      "Iteration 35 \t nan nan\n",
      "Iteration 36 \t nan nan\n",
      "Iteration 37 \t nan nan\n",
      "Iteration 38 \t nan nan\n",
      "Iteration 39 \t nan nan\n",
      "Iteration 40 \t nan nan\n",
      "Iteration 41 \t nan nan\n",
      "Iteration 42 \t nan nan\n",
      "Iteration 43 \t nan nan\n",
      "Iteration 44 \t nan nan\n",
      "Iteration 45 \t nan nan\n",
      "Iteration 46 \t nan nan\n",
      "Iteration 47 \t nan nan\n",
      "Iteration 48 \t nan nan\n",
      "Iteration 49 \t nan nan\n",
      "Iteration 50 \t nan nan\n",
      "Iteration 51 \t nan nan\n",
      "Iteration 52 \t nan nan\n",
      "Iteration 53 \t nan nan\n",
      "Iteration 54 \t nan nan\n",
      "Iteration 55 \t nan nan\n",
      "Iteration 56 \t nan nan\n",
      "Iteration 57 \t nan nan\n",
      "Iteration 58 \t nan nan\n",
      "Iteration 59 \t nan nan\n",
      "Iteration 60 \t nan nan\n",
      "Iteration 61 \t nan nan\n",
      "Iteration 62 \t nan nan\n",
      "Iteration 63 \t nan nan\n",
      "Iteration 64 \t nan nan\n",
      "Iteration 65 \t nan nan\n",
      "Iteration 66 \t nan nan\n",
      "Iteration 67 \t nan nan\n",
      "Iteration 68 \t nan nan\n",
      "Iteration 69 \t nan nan\n",
      "Iteration 70 \t nan nan\n",
      "Iteration 71 \t nan nan\n",
      "Iteration 72 \t nan nan\n",
      "Iteration 73 \t nan nan\n",
      "Iteration 74 \t nan nan\n",
      "Iteration 75 \t nan nan\n",
      "Iteration 76 \t nan nan\n",
      "Iteration 77 \t nan nan\n",
      "Iteration 78 \t nan nan\n",
      "Iteration 79 \t nan nan\n",
      "Iteration 80 \t nan nan\n",
      "Iteration 81 \t nan nan\n",
      "Iteration 82 \t nan nan\n",
      "Iteration 83 \t nan nan\n",
      "Iteration 84 \t nan nan\n",
      "Iteration 85 \t nan nan\n",
      "Iteration 86 \t nan nan\n",
      "Iteration 87 \t nan nan\n",
      "Iteration 88 \t nan nan\n",
      "Iteration 89 \t nan nan\n",
      "Iteration 90 \t nan nan\n",
      "Iteration 91 \t nan nan\n",
      "Iteration 92 \t nan nan\n",
      "Iteration 93 \t nan nan\n",
      "Iteration 94 \t nan nan\n",
      "Iteration 95 \t nan nan\n",
      "Iteration 96 \t nan nan\n",
      "Iteration 97 \t nan nan\n",
      "Iteration 98 \t nan nan\n",
      "Iteration 99 \t nan nan\n",
      "Iteration 100 \t nan nan\n",
      "Iteration 101 \t nan nan\n",
      "Iteration 102 \t nan nan\n",
      "Iteration 103 \t nan nan\n",
      "Iteration 104 \t nan nan\n",
      "Iteration 105 \t nan nan\n",
      "Iteration 106 \t nan nan\n",
      "Iteration 107 \t nan nan\n",
      "Iteration 108 \t nan nan\n",
      "Iteration 109 \t nan nan\n",
      "Iteration 110 \t nan nan\n",
      "Iteration 111 \t nan nan\n",
      "Iteration 112 \t nan nan\n",
      "Iteration 113 \t nan nan\n",
      "Iteration 114 \t nan nan\n",
      "Iteration 115 \t nan nan\n",
      "Iteration 116 \t nan nan\n",
      "Iteration 117 \t nan nan\n",
      "Iteration 118 \t nan nan\n",
      "Iteration 119 \t nan nan\n",
      "Iteration 120 \t nan nan\n",
      "Iteration 121 \t nan nan\n",
      "Iteration 122 \t nan nan\n",
      "Iteration 123 \t nan nan\n",
      "Iteration 124 \t nan nan\n",
      "Iteration 125 \t nan nan\n",
      "Iteration 126 \t nan nan\n",
      "Iteration 127 \t nan nan\n",
      "Iteration 128 \t nan nan\n",
      "Iteration 129 \t nan nan\n",
      "Iteration 130 \t nan nan\n",
      "Iteration 131 \t nan nan\n",
      "Iteration 132 \t nan nan\n",
      "Iteration 133 \t nan nan\n",
      "Iteration 134 \t nan nan\n",
      "Iteration 135 \t nan nan\n",
      "Iteration 136 \t nan nan\n",
      "Iteration 137 \t nan nan\n",
      "Iteration 138 \t nan nan\n",
      "Iteration 139 \t nan nan\n",
      "Iteration 140 \t nan nan\n",
      "Iteration 141 \t nan nan\n",
      "Iteration 142 \t nan nan\n",
      "Iteration 143 \t nan nan\n",
      "Iteration 144 \t nan nan\n",
      "Iteration 145 \t nan nan\n",
      "Iteration 146 \t nan nan\n",
      "Iteration 147 \t nan nan\n",
      "Iteration 148 \t nan nan\n",
      "Iteration 149 \t nan nan\n",
      "Iteration 150 \t nan nan\n",
      "Iteration 151 \t nan nan\n",
      "Iteration 152 \t nan nan\n",
      "Iteration 153 \t nan nan\n",
      "Iteration 154 \t nan nan\n",
      "Iteration 155 \t nan nan\n",
      "Iteration 156 \t nan nan\n",
      "Iteration 157 \t nan nan\n",
      "Iteration 158 \t nan nan\n",
      "Iteration 159 \t nan nan\n",
      "Iteration 160 \t nan nan\n",
      "Iteration 161 \t nan nan\n",
      "Iteration 162 \t nan nan\n",
      "Iteration 163 \t nan nan\n",
      "Iteration 164 \t nan nan\n",
      "Iteration 165 \t nan nan\n",
      "Iteration 166 \t nan nan\n",
      "Iteration 167 \t nan nan\n",
      "Iteration 168 \t nan nan\n",
      "Iteration 169 \t nan nan\n",
      "Iteration 170 \t nan nan\n",
      "Iteration 171 \t nan nan\n",
      "Iteration 172 \t nan nan\n",
      "Iteration 173 \t nan nan\n",
      "Iteration 174 \t nan nan\n",
      "Iteration 175 \t nan nan\n",
      "Iteration 176 \t nan nan\n",
      "Iteration 177 \t nan nan\n",
      "Iteration 178 \t nan nan\n",
      "Iteration 179 \t nan nan\n",
      "Iteration 180 \t nan nan\n",
      "Iteration 181 \t nan nan\n",
      "Iteration 182 \t nan nan\n",
      "Iteration 183 \t nan nan\n",
      "Iteration 184 \t nan nan\n",
      "Iteration 185 \t nan nan\n",
      "Iteration 186 \t nan nan\n",
      "Iteration 187 \t nan nan\n",
      "Iteration 188 \t nan nan\n",
      "Iteration 189 \t nan nan\n",
      "Iteration 190 \t nan nan\n",
      "Iteration 191 \t nan nan\n",
      "Iteration 192 \t nan nan\n",
      "Iteration 193 \t nan nan\n",
      "Iteration 194 \t nan nan\n",
      "Iteration 195 \t nan nan\n",
      "Iteration 196 \t nan nan\n",
      "Iteration 197 \t nan nan\n",
      "Iteration 198 \t nan nan\n",
      "Iteration 199 \t nan nan\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "exp_history = []\n",
    "\n",
    "# OPTIONAL: use an optimizer (stochastic gradient descent)\n",
    "opt = torch.optim.SGD([exp_hat], lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# training loop\n",
    "for i in range(200):\n",
    "    # IF using optimizer\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # Compute the current estimate\n",
    "    y_hat = forward(x, exp_hat)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = rmse(y, y_hat)\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # IF using optimizer\n",
    "    opt.step()\n",
    "    \n",
    "    print('Iteration', i, '\\t', loss.data[0], exp_hat.data[0])\n",
    "    \n",
    "    # Update params\n",
    "    exp_hat.data -= learning_rate * exp_hat.grad.data\n",
    "    exp_hat.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure why I'm only getting nan/inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square((y - y_hat))))\n",
    "\n",
    "def forward(x, e):\n",
    "    return tf.pow(x, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# placeholders for inputs/outputs\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "exp = tf.constant(2.0)\n",
    "exp_hat = tf.Variable(4.0, name='exp_hat')\n",
    "\n",
    "y_hat = forward(x, exp_hat)\n",
    "\n",
    "loss = rmse(y, y_hat)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# Perform a single training step -- equivalent to opt.step() in torch\n",
    "# This also updates model parameters (since it's using a static graph,\n",
    "# just performing the operations on the graph, don't need to update it\n",
    "# ourselves on a new graph).\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "x_train = np.random.rand(n) + 10\n",
    "y_train = x_train ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-78-b1332f8629df>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 12161.8 -1.44604e+11\n",
      "1 110.464 -1.44604e+11\n",
      "2 110.464 -1.44604e+11\n",
      "3 110.464 -1.44604e+11\n",
      "4 110.464 -1.44604e+11\n",
      "5 110.464 -1.44604e+11\n",
      "6 110.464 -1.44604e+11\n",
      "7 110.464 -1.44604e+11\n",
      "8 110.464 -1.44604e+11\n",
      "9 110.464 -1.44604e+11\n",
      "10 110.464 -1.44604e+11\n",
      "11 110.464 -1.44604e+11\n",
      "12 110.464 -1.44604e+11\n",
      "13 110.464 -1.44604e+11\n",
      "14 110.464 -1.44604e+11\n",
      "15 110.464 -1.44604e+11\n",
      "16 110.464 -1.44604e+11\n",
      "17 110.464 -1.44604e+11\n",
      "18 110.464 -1.44604e+11\n",
      "19 110.464 -1.44604e+11\n",
      "20 110.464 -1.44604e+11\n",
      "21 110.464 -1.44604e+11\n",
      "22 110.464 -1.44604e+11\n",
      "23 110.464 -1.44604e+11\n",
      "24 110.464 -1.44604e+11\n",
      "25 110.464 -1.44604e+11\n",
      "26 110.464 -1.44604e+11\n",
      "27 110.464 -1.44604e+11\n",
      "28 110.464 -1.44604e+11\n",
      "29 110.464 -1.44604e+11\n",
      "30 110.464 -1.44604e+11\n",
      "31 110.464 -1.44604e+11\n",
      "32 110.464 -1.44604e+11\n",
      "33 110.464 -1.44604e+11\n",
      "34 110.464 -1.44604e+11\n",
      "35 110.464 -1.44604e+11\n",
      "36 110.464 -1.44604e+11\n",
      "37 110.464 -1.44604e+11\n",
      "38 110.464 -1.44604e+11\n",
      "39 110.464 -1.44604e+11\n",
      "40 110.464 -1.44604e+11\n",
      "41 110.464 -1.44604e+11\n",
      "42 110.464 -1.44604e+11\n",
      "43 110.464 -1.44604e+11\n",
      "44 110.464 -1.44604e+11\n",
      "45 110.464 -1.44604e+11\n",
      "46 110.464 -1.44604e+11\n",
      "47 110.464 -1.44604e+11\n",
      "48 110.464 -1.44604e+11\n",
      "49 110.464 -1.44604e+11\n",
      "50 110.464 -1.44604e+11\n",
      "51 110.464 -1.44604e+11\n",
      "52 110.464 -1.44604e+11\n",
      "53 110.464 -1.44604e+11\n",
      "54 110.464 -1.44604e+11\n",
      "55 110.464 -1.44604e+11\n",
      "56 110.464 -1.44604e+11\n",
      "57 110.464 -1.44604e+11\n",
      "58 110.464 -1.44604e+11\n",
      "59 110.464 -1.44604e+11\n",
      "60 110.464 -1.44604e+11\n",
      "61 110.464 -1.44604e+11\n",
      "62 110.464 -1.44604e+11\n",
      "63 110.464 -1.44604e+11\n",
      "64 110.464 -1.44604e+11\n",
      "65 110.464 -1.44604e+11\n",
      "66 110.464 -1.44604e+11\n",
      "67 110.464 -1.44604e+11\n",
      "68 110.464 -1.44604e+11\n",
      "69 110.464 -1.44604e+11\n",
      "70 110.464 -1.44604e+11\n",
      "71 110.464 -1.44604e+11\n",
      "72 110.464 -1.44604e+11\n",
      "73 110.464 -1.44604e+11\n",
      "74 110.464 -1.44604e+11\n",
      "75 110.464 -1.44604e+11\n",
      "76 110.464 -1.44604e+11\n",
      "77 110.464 -1.44604e+11\n",
      "78 110.464 -1.44604e+11\n",
      "79 110.464 -1.44604e+11\n",
      "80 110.464 -1.44604e+11\n",
      "81 110.464 -1.44604e+11\n",
      "82 110.464 -1.44604e+11\n",
      "83 110.464 -1.44604e+11\n",
      "84 110.464 -1.44604e+11\n",
      "85 110.464 -1.44604e+11\n",
      "86 110.464 -1.44604e+11\n",
      "87 110.464 -1.44604e+11\n",
      "88 110.464 -1.44604e+11\n",
      "89 110.464 -1.44604e+11\n",
      "90 110.464 -1.44604e+11\n",
      "91 110.464 -1.44604e+11\n",
      "92 110.464 -1.44604e+11\n",
      "93 110.464 -1.44604e+11\n",
      "94 110.464 -1.44604e+11\n",
      "95 110.464 -1.44604e+11\n",
      "96 110.464 -1.44604e+11\n",
      "97 110.464 -1.44604e+11\n",
      "98 110.464 -1.44604e+11\n",
      "99 110.464 -1.44604e+11\n",
      "100 110.464 -1.44604e+11\n",
      "101 110.464 -1.44604e+11\n",
      "102 110.464 -1.44604e+11\n",
      "103 110.464 -1.44604e+11\n",
      "104 110.464 -1.44604e+11\n",
      "105 110.464 -1.44604e+11\n",
      "106 110.464 -1.44604e+11\n",
      "107 110.464 -1.44604e+11\n",
      "108 110.464 -1.44604e+11\n",
      "109 110.464 -1.44604e+11\n",
      "110 110.464 -1.44604e+11\n",
      "111 110.464 -1.44604e+11\n",
      "112 110.464 -1.44604e+11\n",
      "113 110.464 -1.44604e+11\n",
      "114 110.464 -1.44604e+11\n",
      "115 110.464 -1.44604e+11\n",
      "116 110.464 -1.44604e+11\n",
      "117 110.464 -1.44604e+11\n",
      "118 110.464 -1.44604e+11\n",
      "119 110.464 -1.44604e+11\n",
      "120 110.464 -1.44604e+11\n",
      "121 110.464 -1.44604e+11\n",
      "122 110.464 -1.44604e+11\n",
      "123 110.464 -1.44604e+11\n",
      "124 110.464 -1.44604e+11\n",
      "125 110.464 -1.44604e+11\n",
      "126 110.464 -1.44604e+11\n",
      "127 110.464 -1.44604e+11\n",
      "128 110.464 -1.44604e+11\n",
      "129 110.464 -1.44604e+11\n",
      "130 110.464 -1.44604e+11\n",
      "131 110.464 -1.44604e+11\n",
      "132 110.464 -1.44604e+11\n",
      "133 110.464 -1.44604e+11\n",
      "134 110.464 -1.44604e+11\n",
      "135 110.464 -1.44604e+11\n",
      "136 110.464 -1.44604e+11\n",
      "137 110.464 -1.44604e+11\n",
      "138 110.464 -1.44604e+11\n",
      "139 110.464 -1.44604e+11\n",
      "140 110.464 -1.44604e+11\n",
      "141 110.464 -1.44604e+11\n",
      "142 110.464 -1.44604e+11\n",
      "143 110.464 -1.44604e+11\n",
      "144 110.464 -1.44604e+11\n",
      "145 110.464 -1.44604e+11\n",
      "146 110.464 -1.44604e+11\n",
      "147 110.464 -1.44604e+11\n",
      "148 110.464 -1.44604e+11\n",
      "149 110.464 -1.44604e+11\n",
      "150 110.464 -1.44604e+11\n",
      "151 110.464 -1.44604e+11\n",
      "152 110.464 -1.44604e+11\n",
      "153 110.464 -1.44604e+11\n",
      "154 110.464 -1.44604e+11\n",
      "155 110.464 -1.44604e+11\n",
      "156 110.464 -1.44604e+11\n",
      "157 110.464 -1.44604e+11\n",
      "158 110.464 -1.44604e+11\n",
      "159 110.464 -1.44604e+11\n",
      "160 110.464 -1.44604e+11\n",
      "161 110.464 -1.44604e+11\n",
      "162 110.464 -1.44604e+11\n",
      "163 110.464 -1.44604e+11\n",
      "164 110.464 -1.44604e+11\n",
      "165 110.464 -1.44604e+11\n",
      "166 110.464 -1.44604e+11\n",
      "167 110.464 -1.44604e+11\n",
      "168 110.464 -1.44604e+11\n",
      "169 110.464 -1.44604e+11\n",
      "170 110.464 -1.44604e+11\n",
      "171 110.464 -1.44604e+11\n",
      "172 110.464 -1.44604e+11\n",
      "173 110.464 -1.44604e+11\n",
      "174 110.464 -1.44604e+11\n",
      "175 110.464 -1.44604e+11\n",
      "176 110.464 -1.44604e+11\n",
      "177 110.464 -1.44604e+11\n",
      "178 110.464 -1.44604e+11\n",
      "179 110.464 -1.44604e+11\n",
      "180 110.464 -1.44604e+11\n",
      "181 110.464 -1.44604e+11\n",
      "182 110.464 -1.44604e+11\n",
      "183 110.464 -1.44604e+11\n",
      "184 110.464 -1.44604e+11\n",
      "185 110.464 -1.44604e+11\n",
      "186 110.464 -1.44604e+11\n",
      "187 110.464 -1.44604e+11\n",
      "188 110.464 -1.44604e+11\n",
      "189 110.464 -1.44604e+11\n",
      "190 110.464 -1.44604e+11\n",
      "191 110.464 -1.44604e+11\n",
      "192 110.464 -1.44604e+11\n",
      "193 110.464 -1.44604e+11\n",
      "194 110.464 -1.44604e+11\n",
      "195 110.464 -1.44604e+11\n",
      "196 110.464 -1.44604e+11\n",
      "197 110.464 -1.44604e+11\n",
      "198 110.464 -1.44604e+11\n",
      "199 110.464 -1.44604e+11\n",
      "200 110.464 -1.44604e+11\n",
      "201 110.464 -1.44604e+11\n",
      "202 110.464 -1.44604e+11\n",
      "203 110.464 -1.44604e+11\n",
      "204 110.464 -1.44604e+11\n",
      "205 110.464 -1.44604e+11\n",
      "206 110.464 -1.44604e+11\n",
      "207 110.464 -1.44604e+11\n",
      "208 110.464 -1.44604e+11\n",
      "209 110.464 -1.44604e+11\n",
      "210 110.464 -1.44604e+11\n",
      "211 110.464 -1.44604e+11\n",
      "212 110.464 -1.44604e+11\n",
      "213 110.464 -1.44604e+11\n",
      "214 110.464 -1.44604e+11\n",
      "215 110.464 -1.44604e+11\n",
      "216 110.464 -1.44604e+11\n",
      "217 110.464 -1.44604e+11\n",
      "218 110.464 -1.44604e+11\n",
      "219 110.464 -1.44604e+11\n",
      "220 110.464 -1.44604e+11\n",
      "221 110.464 -1.44604e+11\n",
      "222 110.464 -1.44604e+11\n",
      "223 110.464 -1.44604e+11\n",
      "224 110.464 -1.44604e+11\n",
      "225 110.464 -1.44604e+11\n",
      "226 110.464 -1.44604e+11\n",
      "227 110.464 -1.44604e+11\n",
      "228 110.464 -1.44604e+11\n",
      "229 110.464 -1.44604e+11\n",
      "230 110.464 -1.44604e+11\n",
      "231 110.464 -1.44604e+11\n",
      "232 110.464 -1.44604e+11\n",
      "233 110.464 -1.44604e+11\n",
      "234 110.464 -1.44604e+11\n",
      "235 110.464 -1.44604e+11\n",
      "236 110.464 -1.44604e+11\n",
      "237 110.464 -1.44604e+11\n",
      "238 110.464 -1.44604e+11\n",
      "239 110.464 -1.44604e+11\n",
      "240 110.464 -1.44604e+11\n",
      "241 110.464 -1.44604e+11\n",
      "242 110.464 -1.44604e+11\n",
      "243 110.464 -1.44604e+11\n",
      "244 110.464 -1.44604e+11\n",
      "245 110.464 -1.44604e+11\n",
      "246 110.464 -1.44604e+11\n",
      "247 110.464 -1.44604e+11\n",
      "248 110.464 -1.44604e+11\n",
      "249 110.464 -1.44604e+11\n",
      "250 110.464 -1.44604e+11\n",
      "251 110.464 -1.44604e+11\n",
      "252 110.464 -1.44604e+11\n",
      "253 110.464 -1.44604e+11\n",
      "254 110.464 -1.44604e+11\n",
      "255 110.464 -1.44604e+11\n",
      "256 110.464 -1.44604e+11\n",
      "257 110.464 -1.44604e+11\n",
      "258 110.464 -1.44604e+11\n",
      "259 110.464 -1.44604e+11\n",
      "260 110.464 -1.44604e+11\n",
      "261 110.464 -1.44604e+11\n",
      "262 110.464 -1.44604e+11\n",
      "263 110.464 -1.44604e+11\n",
      "264 110.464 -1.44604e+11\n",
      "265 110.464 -1.44604e+11\n",
      "266 110.464 -1.44604e+11\n",
      "267 110.464 -1.44604e+11\n",
      "268 110.464 -1.44604e+11\n",
      "269 110.464 -1.44604e+11\n",
      "270 110.464 -1.44604e+11\n",
      "271 110.464 -1.44604e+11\n",
      "272 110.464 -1.44604e+11\n",
      "273 110.464 -1.44604e+11\n",
      "274 110.464 -1.44604e+11\n",
      "275 110.464 -1.44604e+11\n",
      "276 110.464 -1.44604e+11\n",
      "277 110.464 -1.44604e+11\n",
      "278 110.464 -1.44604e+11\n",
      "279 110.464 -1.44604e+11\n",
      "280 110.464 -1.44604e+11\n",
      "281 110.464 -1.44604e+11\n",
      "282 110.464 -1.44604e+11\n",
      "283 110.464 -1.44604e+11\n",
      "284 110.464 -1.44604e+11\n",
      "285 110.464 -1.44604e+11\n",
      "286 110.464 -1.44604e+11\n",
      "287 110.464 -1.44604e+11\n",
      "288 110.464 -1.44604e+11\n",
      "289 110.464 -1.44604e+11\n",
      "290 110.464 -1.44604e+11\n",
      "291 110.464 -1.44604e+11\n",
      "292 110.464 -1.44604e+11\n",
      "293 110.464 -1.44604e+11\n",
      "294 110.464 -1.44604e+11\n",
      "295 110.464 -1.44604e+11\n",
      "296 110.464 -1.44604e+11\n",
      "297 110.464 -1.44604e+11\n",
      "298 110.464 -1.44604e+11\n",
      "299 110.464 -1.44604e+11\n",
      "300 110.464 -1.44604e+11\n",
      "301 110.464 -1.44604e+11\n",
      "302 110.464 -1.44604e+11\n",
      "303 110.464 -1.44604e+11\n",
      "304 110.464 -1.44604e+11\n",
      "305 110.464 -1.44604e+11\n",
      "306 110.464 -1.44604e+11\n",
      "307 110.464 -1.44604e+11\n",
      "308 110.464 -1.44604e+11\n",
      "309 110.464 -1.44604e+11\n",
      "310 110.464 -1.44604e+11\n",
      "311 110.464 -1.44604e+11\n",
      "312 110.464 -1.44604e+11\n",
      "313 110.464 -1.44604e+11\n",
      "314 110.464 -1.44604e+11\n",
      "315 110.464 -1.44604e+11\n",
      "316 110.464 -1.44604e+11\n",
      "317 110.464 -1.44604e+11\n",
      "318 110.464 -1.44604e+11\n",
      "319 110.464 -1.44604e+11\n",
      "320 110.464 -1.44604e+11\n",
      "321 110.464 -1.44604e+11\n",
      "322 110.464 -1.44604e+11\n",
      "323 110.464 -1.44604e+11\n",
      "324 110.464 -1.44604e+11\n",
      "325 110.464 -1.44604e+11\n",
      "326 110.464 -1.44604e+11\n",
      "327 110.464 -1.44604e+11\n",
      "328 110.464 -1.44604e+11\n",
      "329 110.464 -1.44604e+11\n",
      "330 110.464 -1.44604e+11\n",
      "331 110.464 -1.44604e+11\n",
      "332 110.464 -1.44604e+11\n",
      "333 110.464 -1.44604e+11\n",
      "334 110.464 -1.44604e+11\n",
      "335 110.464 -1.44604e+11\n",
      "336 110.464 -1.44604e+11\n",
      "337 110.464 -1.44604e+11\n",
      "338 110.464 -1.44604e+11\n",
      "339 110.464 -1.44604e+11\n",
      "340 110.464 -1.44604e+11\n",
      "341 110.464 -1.44604e+11\n",
      "342 110.464 -1.44604e+11\n",
      "343 110.464 -1.44604e+11\n",
      "344 110.464 -1.44604e+11\n",
      "345 110.464 -1.44604e+11\n",
      "346 110.464 -1.44604e+11\n",
      "347 110.464 -1.44604e+11\n",
      "348 110.464 -1.44604e+11\n",
      "349 110.464 -1.44604e+11\n",
      "350 110.464 -1.44604e+11\n",
      "351 110.464 -1.44604e+11\n",
      "352 110.464 -1.44604e+11\n",
      "353 110.464 -1.44604e+11\n",
      "354 110.464 -1.44604e+11\n",
      "355 110.464 -1.44604e+11\n",
      "356 110.464 -1.44604e+11\n",
      "357 110.464 -1.44604e+11\n",
      "358 110.464 -1.44604e+11\n",
      "359 110.464 -1.44604e+11\n",
      "360 110.464 -1.44604e+11\n",
      "361 110.464 -1.44604e+11\n",
      "362 110.464 -1.44604e+11\n",
      "363 110.464 -1.44604e+11\n",
      "364 110.464 -1.44604e+11\n",
      "365 110.464 -1.44604e+11\n",
      "366 110.464 -1.44604e+11\n",
      "367 110.464 -1.44604e+11\n",
      "368 110.464 -1.44604e+11\n",
      "369 110.464 -1.44604e+11\n",
      "370 110.464 -1.44604e+11\n",
      "371 110.464 -1.44604e+11\n",
      "372 110.464 -1.44604e+11\n",
      "373 110.464 -1.44604e+11\n",
      "374 110.464 -1.44604e+11\n",
      "375 110.464 -1.44604e+11\n",
      "376 110.464 -1.44604e+11\n",
      "377 110.464 -1.44604e+11\n",
      "378 110.464 -1.44604e+11\n",
      "379 110.464 -1.44604e+11\n",
      "380 110.464 -1.44604e+11\n",
      "381 110.464 -1.44604e+11\n",
      "382 110.464 -1.44604e+11\n",
      "383 110.464 -1.44604e+11\n",
      "384 110.464 -1.44604e+11\n",
      "385 110.464 -1.44604e+11\n",
      "386 110.464 -1.44604e+11\n",
      "387 110.464 -1.44604e+11\n",
      "388 110.464 -1.44604e+11\n",
      "389 110.464 -1.44604e+11\n",
      "390 110.464 -1.44604e+11\n",
      "391 110.464 -1.44604e+11\n",
      "392 110.464 -1.44604e+11\n",
      "393 110.464 -1.44604e+11\n",
      "394 110.464 -1.44604e+11\n",
      "395 110.464 -1.44604e+11\n",
      "396 110.464 -1.44604e+11\n",
      "397 110.464 -1.44604e+11\n",
      "398 110.464 -1.44604e+11\n",
      "399 110.464 -1.44604e+11\n",
      "400 110.464 -1.44604e+11\n",
      "401 110.464 -1.44604e+11\n",
      "402 110.464 -1.44604e+11\n",
      "403 110.464 -1.44604e+11\n",
      "404 110.464 -1.44604e+11\n",
      "405 110.464 -1.44604e+11\n",
      "406 110.464 -1.44604e+11\n",
      "407 110.464 -1.44604e+11\n",
      "408 110.464 -1.44604e+11\n",
      "409 110.464 -1.44604e+11\n",
      "410 110.464 -1.44604e+11\n",
      "411 110.464 -1.44604e+11\n",
      "412 110.464 -1.44604e+11\n",
      "413 110.464 -1.44604e+11\n",
      "414 110.464 -1.44604e+11\n",
      "415 110.464 -1.44604e+11\n",
      "416 110.464 -1.44604e+11\n",
      "417 110.464 -1.44604e+11\n",
      "418 110.464 -1.44604e+11\n",
      "419 110.464 -1.44604e+11\n",
      "420 110.464 -1.44604e+11\n",
      "421 110.464 -1.44604e+11\n",
      "422 110.464 -1.44604e+11\n",
      "423 110.464 -1.44604e+11\n",
      "424 110.464 -1.44604e+11\n",
      "425 110.464 -1.44604e+11\n",
      "426 110.464 -1.44604e+11\n",
      "427 110.464 -1.44604e+11\n",
      "428 110.464 -1.44604e+11\n",
      "429 110.464 -1.44604e+11\n",
      "430 110.464 -1.44604e+11\n",
      "431 110.464 -1.44604e+11\n",
      "432 110.464 -1.44604e+11\n",
      "433 110.464 -1.44604e+11\n",
      "434 110.464 -1.44604e+11\n",
      "435 110.464 -1.44604e+11\n",
      "436 110.464 -1.44604e+11\n",
      "437 110.464 -1.44604e+11\n",
      "438 110.464 -1.44604e+11\n",
      "439 110.464 -1.44604e+11\n",
      "440 110.464 -1.44604e+11\n",
      "441 110.464 -1.44604e+11\n",
      "442 110.464 -1.44604e+11\n",
      "443 110.464 -1.44604e+11\n",
      "444 110.464 -1.44604e+11\n",
      "445 110.464 -1.44604e+11\n",
      "446 110.464 -1.44604e+11\n",
      "447 110.464 -1.44604e+11\n",
      "448 110.464 -1.44604e+11\n",
      "449 110.464 -1.44604e+11\n",
      "450 110.464 -1.44604e+11\n",
      "451 110.464 -1.44604e+11\n",
      "452 110.464 -1.44604e+11\n",
      "453 110.464 -1.44604e+11\n",
      "454 110.464 -1.44604e+11\n",
      "455 110.464 -1.44604e+11\n",
      "456 110.464 -1.44604e+11\n",
      "457 110.464 -1.44604e+11\n",
      "458 110.464 -1.44604e+11\n",
      "459 110.464 -1.44604e+11\n",
      "460 110.464 -1.44604e+11\n",
      "461 110.464 -1.44604e+11\n",
      "462 110.464 -1.44604e+11\n",
      "463 110.464 -1.44604e+11\n",
      "464 110.464 -1.44604e+11\n",
      "465 110.464 -1.44604e+11\n",
      "466 110.464 -1.44604e+11\n",
      "467 110.464 -1.44604e+11\n",
      "468 110.464 -1.44604e+11\n",
      "469 110.464 -1.44604e+11\n",
      "470 110.464 -1.44604e+11\n",
      "471 110.464 -1.44604e+11\n",
      "472 110.464 -1.44604e+11\n",
      "473 110.464 -1.44604e+11\n",
      "474 110.464 -1.44604e+11\n",
      "475 110.464 -1.44604e+11\n",
      "476 110.464 -1.44604e+11\n",
      "477 110.464 -1.44604e+11\n",
      "478 110.464 -1.44604e+11\n",
      "479 110.464 -1.44604e+11\n",
      "480 110.464 -1.44604e+11\n",
      "481 110.464 -1.44604e+11\n",
      "482 110.464 -1.44604e+11\n",
      "483 110.464 -1.44604e+11\n",
      "484 110.464 -1.44604e+11\n",
      "485 110.464 -1.44604e+11\n",
      "486 110.464 -1.44604e+11\n",
      "487 110.464 -1.44604e+11\n",
      "488 110.464 -1.44604e+11\n",
      "489 110.464 -1.44604e+11\n",
      "490 110.464 -1.44604e+11\n",
      "491 110.464 -1.44604e+11\n",
      "492 110.464 -1.44604e+11\n",
      "493 110.464 -1.44604e+11\n",
      "494 110.464 -1.44604e+11\n",
      "495 110.464 -1.44604e+11\n",
      "496 110.464 -1.44604e+11\n",
      "497 110.464 -1.44604e+11\n",
      "498 110.464 -1.44604e+11\n",
      "499 110.464 -1.44604e+11\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(500):\n",
    "        curr_loss, curr_exp, _ = session.run([loss, exp_hat, train_op], feed_dict={x: x_train, y: y_train})\n",
    "        \n",
    "        print(i, curr_loss, curr_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wtf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
